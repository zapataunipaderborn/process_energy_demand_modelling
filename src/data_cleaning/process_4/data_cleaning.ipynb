{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 50px;\">\n",
    "    <b>Energy and Sensor mapping on Process Data</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [Define Paths and Load Datasets](#define-paths-and-load-datasets)\n",
    "    - [Define Paths](#define-paths)\n",
    "    - [Load Datasets](#load-datasets)\n",
    "    - [Data Transformation and Cleaning](#data-transformation-and-cleaning)\n",
    "- [Merge Datasets](#merge-datasets)\n",
    "- [Map Energy Data to Activities](#map-energy-data-to-activities)\n",
    "    - [Map Electricity & Steam to each Activity](#map-electricity--steam-to-each-activity)\n",
    "        - [Electricity](#electricity)\n",
    "        - [Steam](#steam)\n",
    "    - [Calculate Electricity and Steam Consmption for the whole Process](#calculate-electricity-and-steam-consumption-for-the-whole-process)\n",
    "- [Merge Sensor Data](#merge-sensor-data)\n",
    "    - [Merge Sensor Data on Activity Level](#merge-sensor-data-on-activity-level)\n",
    "    - [Merge Sensor Data on Process Level](#merge-sensor-data-on-process-level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import itertools\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "#import simulation\n",
    "import simpy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "import plotly.io as pio\n",
    "from pathlib import Path\n",
    "import pm4py\n",
    "pio.renderers.default='notebook'\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths and Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the current file's location\n",
    "current_path = Path(__file__).resolve().parent if '__file__' in globals() else Path().resolve()\n",
    "\n",
    "# Navigate to the 'data/bronze/aixit' folder relative to the current path\n",
    "files_folder_bronze = current_path.parent.parent.parent / 'data' / 'bronze' / 'eckes_granini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the current file's location\n",
    "current_path = Path(__file__).resolve().parent if '__file__' in globals() else Path().resolve()\n",
    "\n",
    "# Navigate to the 'data/silver/aixit' folder relative to the current path\n",
    "files_folder_silver = current_path.parent.parent.parent   / 'data' / 'silver' / 'process_4'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['OrderNumber', 'BF Product', 'BF Number', 'StartTime', 'EndTime',\n",
      "       'ActualQuantity', 'SFG Product', 'SFG Brix-Density', 'Unnamed: 8'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "case_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "activity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "timestamp_start",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "timestamp_end",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "object",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "object_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "higher_level_activity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "object_attributes",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "11d42a3b-4883-4ac8-a3d9-d40b1e6a9490",
       "rows": [
        [
         "0",
         "10705690",
         "production",
         "2024-08-01 07:41:45",
         "2024-08-01 13:28:09",
         "material_flow",
         "order",
         null,
         "{'attr_product': 'BF 1,00 GRTG Apfel trüb S DE PET', 'attr_product_id': 4006719, 'attr_quantity': 59176, 'attr_sfg_product_info': 4501437, 'attr_sfg_brix_density': 11.94}"
        ],
        [
         "1",
         "10705688",
         "production",
         "2024-08-01 13:46:24",
         "2024-08-01 15:47:52",
         "material_flow",
         "order",
         null,
         "{'attr_product': 'BF 1,00 GR WeißeGrapefr N D01 PET Neu', 'attr_product_id': 4003635, 'attr_quantity': 19324, 'attr_sfg_product_info': 4500725, 'attr_sfg_brix_density': 10.42}"
        ],
        [
         "2",
         "10705695",
         "production",
         "2024-08-01 16:21:32",
         "2024-08-01 21:33:50",
         "material_flow",
         "order",
         null,
         "{'attr_product': 'BF 1,00 GR PinkGrapefruit D D01 PET', 'attr_product_id': 4007556, 'attr_quantity': 60104, 'attr_sfg_product_info': 4501667, 'attr_sfg_brix_density': 9.32}"
        ],
        [
         "3",
         "10705698",
         "production",
         "2024-08-01 21:26:12",
         "2024-08-02 00:22:59",
         "material_flow",
         "order",
         null,
         "{'attr_product': 'BF 1,00 GRTG PinkGrapefruit G DE PET', 'attr_product_id': 4007566, 'attr_quantity': 29706, 'attr_sfg_product_info': 4501667, 'attr_sfg_brix_density': 9.32}"
        ],
        [
         "4",
         "10705696",
         "production",
         "2024-08-02 00:43:28",
         "2024-08-02 15:46:51",
         "material_flow",
         "order",
         null,
         "{'attr_product': 'BF 1,00 GRTG Multi 50% N DE PET', 'attr_product_id': 4007561, 'attr_quantity': 177447, 'attr_sfg_product_info': 4501959, 'attr_sfg_brix_density': 11.02}"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp_start</th>\n",
       "      <th>timestamp_end</th>\n",
       "      <th>object</th>\n",
       "      <th>object_type</th>\n",
       "      <th>higher_level_activity</th>\n",
       "      <th>object_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10705690</td>\n",
       "      <td>production</td>\n",
       "      <td>2024-08-01 07:41:45</td>\n",
       "      <td>2024-08-01 13:28:09</td>\n",
       "      <td>material_flow</td>\n",
       "      <td>order</td>\n",
       "      <td>None</td>\n",
       "      <td>{'attr_product': 'BF 1,00 GRTG Apfel trüb S DE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10705688</td>\n",
       "      <td>production</td>\n",
       "      <td>2024-08-01 13:46:24</td>\n",
       "      <td>2024-08-01 15:47:52</td>\n",
       "      <td>material_flow</td>\n",
       "      <td>order</td>\n",
       "      <td>None</td>\n",
       "      <td>{'attr_product': 'BF 1,00 GR WeißeGrapefr N D0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10705695</td>\n",
       "      <td>production</td>\n",
       "      <td>2024-08-01 16:21:32</td>\n",
       "      <td>2024-08-01 21:33:50</td>\n",
       "      <td>material_flow</td>\n",
       "      <td>order</td>\n",
       "      <td>None</td>\n",
       "      <td>{'attr_product': 'BF 1,00 GR PinkGrapefruit D ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10705698</td>\n",
       "      <td>production</td>\n",
       "      <td>2024-08-01 21:26:12</td>\n",
       "      <td>2024-08-02 00:22:59</td>\n",
       "      <td>material_flow</td>\n",
       "      <td>order</td>\n",
       "      <td>None</td>\n",
       "      <td>{'attr_product': 'BF 1,00 GRTG PinkGrapefruit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10705696</td>\n",
       "      <td>production</td>\n",
       "      <td>2024-08-02 00:43:28</td>\n",
       "      <td>2024-08-02 15:46:51</td>\n",
       "      <td>material_flow</td>\n",
       "      <td>order</td>\n",
       "      <td>None</td>\n",
       "      <td>{'attr_product': 'BF 1,00 GRTG Multi 50% N DE ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    case_id    activity     timestamp_start       timestamp_end  \\\n",
       "0  10705690  production 2024-08-01 07:41:45 2024-08-01 13:28:09   \n",
       "1  10705688  production 2024-08-01 13:46:24 2024-08-01 15:47:52   \n",
       "2  10705695  production 2024-08-01 16:21:32 2024-08-01 21:33:50   \n",
       "3  10705698  production 2024-08-01 21:26:12 2024-08-02 00:22:59   \n",
       "4  10705696  production 2024-08-02 00:43:28 2024-08-02 15:46:51   \n",
       "\n",
       "          object object_type higher_level_activity  \\\n",
       "0  material_flow       order                  None   \n",
       "1  material_flow       order                  None   \n",
       "2  material_flow       order                  None   \n",
       "3  material_flow       order                  None   \n",
       "4  material_flow       order                  None   \n",
       "\n",
       "                                   object_attributes  \n",
       "0  {'attr_product': 'BF 1,00 GRTG Apfel trüb S DE...  \n",
       "1  {'attr_product': 'BF 1,00 GR WeißeGrapefr N D0...  \n",
       "2  {'attr_product': 'BF 1,00 GR PinkGrapefruit D ...  \n",
       "3  {'attr_product': 'BF 1,00 GRTG PinkGrapefruit ...  \n",
       "4  {'attr_product': 'BF 1,00 GRTG Multi 50% N DE ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Aufträge PET2 08_2024\n",
    "df = pd.read_excel(files_folder_bronze/\"Aufträge Energie Meldungen/Aufträge PET2 08_2024.xlsx\", header=1, parse_dates=[\"StartTime\", \"EndTime\"])\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "df = df.rename(columns={\n",
    "    \"OrderNumber\": \"case_id\",\n",
    "    \"BF Product\": \"attr_product\",\n",
    "    \"BF Number\": \"attr_product_id\",\n",
    "    \"StartTime\": \"timestamp_start\",\n",
    "    \"EndTime\": \"timestamp_end\",\n",
    "    \"ActualQuantity\": \"attr_quantity\",\n",
    "    \"SFG Product\": \"attr_sfg_product_info\",\n",
    "    \"SFG Brix-Density\": \"attr_sfg_brix_density\",\n",
    "})\n",
    "\n",
    "time_min = df['timestamp_start'].min()\n",
    "time_max = df['timestamp_end'].max()\n",
    "\n",
    "df['object_type'] = 'order'\n",
    "df['higher_level_activity'] = None\n",
    "df['object'] = 'material_flow'\n",
    "df['activity'] = 'production'\n",
    "\n",
    "df['object_attributes'] = df.apply(\n",
    "    lambda row: {col: row[col] for col in df.columns if col.startswith('attr')},\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df = df[['case_id', 'activity', 'timestamp_start', 'timestamp_end', 'object','object_type',  'higher_level_activity', 'object_attributes']]\n",
    "\n",
    "df_object_1 = df.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Objektbezeichnung', 'Kommende Meldung', 'Aufgetreten', 'Gegangen'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "case_id",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "activity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "timestamp_start",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "timestamp_end",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "object",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "object_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "higher_level_activity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "object_attributes",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "0ddea69d-6fa8-4f98-9703-676315d6d366",
       "rows": [
        [
         "1",
         null,
         "CIP",
         "2024-08-30 17:44:18",
         "2024-08-30 17:44:18",
         "Status PET 2 KZE",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "5",
         null,
         "benutzt",
         "2024-08-30 17:08:58",
         "2024-08-30 17:08:58",
         "Status PET 2 KZE",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "6",
         null,
         "KZE_Strang1 - Warnung Differenzdruck Strang 1 AT2 unterschritten",
         "2024-08-30 17:08:51",
         "2024-08-30 17:08:58",
         "Fehler PET 2 KZE",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "7",
         null,
         "KZE_Strang1 - Warnung Differenzdruck Strang 1 KW1 unterschritten",
         "2024-08-30 17:08:50",
         "2024-08-30 17:08:58",
         "Fehler PET 2 KZE",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "8",
         null,
         "KZE_Strang1 - Warnung Differenzdruck Strang 1 KW2 unterschritten",
         "2024-08-30 17:08:45",
         "2024-08-30 17:08:58",
         "Fehler PET 2 KZE",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "9",
         null,
         "Step-047 = Anlage Stop",
         "2024-08-30 17:08:18",
         "2024-08-30 17:08:58",
         "Erhitzer PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "10",
         null,
         "Abfahren",
         "2024-08-30 17:05:46",
         "2024-08-30 17:05:46",
         "Status PET 2 KZE",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "11",
         null,
         "Step-046 = Anlage Abfahren",
         "2024-08-30 17:05:46",
         "2024-08-30 17:08:18",
         "Erhitzer PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "12",
         null,
         "Step-001 = Erstbefuellung",
         "2024-08-30 17:03:46",
         "2024-08-30 17:39:14",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "13",
         null,
         "Step-008 = Entleeren",
         "2024-08-30 17:03:16",
         "2024-08-30 17:03:46",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "14",
         null,
         "Step-007 = Spülen",
         "2024-08-30 17:02:42",
         "2024-08-30 17:03:16",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "15",
         null,
         "Step-008 = Entleeren",
         "2024-08-30 17:02:36",
         "2024-08-30 17:02:42",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "16",
         null,
         "Step-007 = Spülen",
         "2024-08-30 17:02:02",
         "2024-08-30 17:02:36",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "17",
         null,
         "Step-001 = Betrieb",
         "2024-08-30 16:45:48",
         "2024-08-30 17:38:58",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "18",
         null,
         "Step-007 = Entleeren",
         "2024-08-30 16:45:15",
         "2024-08-30 16:45:48",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "19",
         null,
         "Step-006 = Spülen",
         "2024-08-30 16:44:35",
         "2024-08-30 16:45:15",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "20",
         null,
         "Step-007 = Entleeren",
         "2024-08-30 16:43:55",
         "2024-08-30 16:44:35",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "21",
         null,
         "Step-006 = Spülen",
         "2024-08-30 16:43:13",
         "2024-08-30 16:43:55",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "22",
         null,
         "Step-011 = Leitung ausschieben",
         "2024-08-30 16:41:15",
         "2024-08-30 16:43:13",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "23",
         null,
         "Step-010 = Produkt umpumpen",
         "2024-08-30 16:40:35",
         "2024-08-30 16:41:15",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "24",
         null,
         "Step-009 = Füllen Leitung",
         "2024-08-30 16:39:47",
         "2024-08-30 16:40:35",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "25",
         null,
         "Step-020 = Sterilwasserumlauf (über VLG)",
         "2024-08-30 16:38:35",
         "2024-08-30 17:05:46",
         "Erhitzer PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "26",
         null,
         "Steril",
         "2024-08-30 16:38:30",
         "2024-08-30 16:38:30",
         "Status PET 2 KZE",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "27",
         null,
         "Step-020 = Ausschub Ende",
         "2024-08-30 16:38:30",
         "2024-08-30 16:38:35",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "28",
         null,
         "Step-014 = Ausschub Erhitzer Spülen",
         "2024-08-30 16:36:06",
         "2024-08-30 16:38:30",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "29",
         null,
         "Step-012 = Ausschub in Gully (Mischphase Wasser)",
         "2024-08-30 16:36:03",
         "2024-08-30 16:36:06",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "30",
         null,
         "Step-001 = Betrieb",
         "2024-08-30 16:36:03",
         "2024-08-30 16:39:47",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "31",
         null,
         "Step-011 = Ausschub in Rework (Mischphase Produkt)",
         "2024-08-30 16:35:24",
         "2024-08-30 16:36:03",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "32",
         null,
         "Step-010 = Ausschub in Produkttank (ohne Mischphase Produkt/Wasser)",
         "2024-08-30 16:34:44",
         "2024-08-30 16:35:24",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "33",
         null,
         "Step-009 = Ausschub bis Füller/Steriltank 2",
         "2024-08-30 16:30:28",
         "2024-08-30 16:34:44",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "34",
         null,
         "Step-007 = Ausschub Heisshalter",
         "2024-08-30 16:29:39",
         "2024-08-30 16:30:28",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "35",
         null,
         "Step-006 = Ausschub bis Heisshalter",
         "2024-08-30 16:27:47",
         "2024-08-30 16:29:39",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "36",
         null,
         "Step-005 = Ausschub bis Entgaser, Bypassventil",
         "2024-08-30 16:27:24",
         "2024-08-30 16:27:47",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "37",
         null,
         "Step-004 = Entleern Entgaser",
         "2024-08-30 16:27:08",
         "2024-08-30 16:27:24",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "38",
         null,
         "Step-003 = Ausschub bis Entgaser",
         "2024-08-30 16:27:01",
         "2024-08-30 16:27:08",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "39",
         null,
         "Step-002 = Ausschub bis Entgaser, Bypassventil",
         "2024-08-30 16:24:19",
         "2024-08-30 16:27:01",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "40",
         null,
         "Step-001 = Entleeregen Vorlaufgefäss",
         "2024-08-30 16:23:08",
         "2024-08-30 16:24:19",
         "Erhitzer PET 2 Ausschieben",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "41",
         null,
         "Step-035 = Ausschub Anlage",
         "2024-08-30 16:23:08",
         "2024-08-30 16:38:35",
         "Erhitzer PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "42",
         null,
         "Step-002 = Produkt sammeln",
         "2024-08-30 16:23:08",
         "2024-08-30 16:36:03",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "43",
         null,
         "KZE_Allgemein - Leitwert Einlauf während Produktion unterschritten",
         "2024-08-30 16:22:44",
         "2024-08-30 16:23:08",
         "Fehler PET 2 KZE",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "44",
         null,
         "Step-032 = Umlauf",
         "2024-08-30 16:22:44",
         "2024-08-30 16:23:08",
         "Erhitzer PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "45",
         null,
         "Step-001 = Betrieb",
         "2024-08-30 16:14:38",
         "2024-08-30 16:23:08",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "46",
         null,
         "Step-003 = Bereit",
         "2024-08-30 15:12:22",
         "2024-08-30 17:02:02",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "47",
         null,
         "Step-004 = Dosieren",
         "2024-08-30 15:12:12",
         "2024-08-30 16:14:38",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "48",
         null,
         "Step-001 = Betrieb",
         "2024-08-30 15:11:16",
         "2024-08-30 15:12:12",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "49",
         null,
         "Step-004 = Betrieb in Reworktank",
         "2024-08-30 15:11:08",
         "2024-08-30 15:12:22",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "50",
         null,
         "Step-003 = Umwälzen",
         "2024-08-30 15:09:11",
         "2024-08-30 15:11:16",
         "Rework PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "51",
         null,
         "Step-003 = Bereit",
         "2024-08-30 15:09:00",
         "2024-08-30 15:11:08",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "52",
         null,
         "Step-006 = Betrieb auf Gully",
         "2024-08-30 15:08:28",
         "2024-08-30 15:09:00",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ],
        [
         "53",
         null,
         "Step-003 = Bereit",
         "2024-08-30 15:05:56",
         "2024-08-30 15:08:28",
         "Auslaugfefäß PET 2",
         "heating_machine",
         "production",
         "{}"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 7719
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp_start</th>\n",
       "      <th>timestamp_end</th>\n",
       "      <th>object</th>\n",
       "      <th>object_type</th>\n",
       "      <th>higher_level_activity</th>\n",
       "      <th>object_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>CIP</td>\n",
       "      <td>2024-08-30 17:44:18</td>\n",
       "      <td>2024-08-30 17:44:18</td>\n",
       "      <td>Status PET 2 KZE</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>benutzt</td>\n",
       "      <td>2024-08-30 17:08:58</td>\n",
       "      <td>2024-08-30 17:08:58</td>\n",
       "      <td>Status PET 2 KZE</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>KZE_Strang1 - Warnung Differenzdruck Strang 1 ...</td>\n",
       "      <td>2024-08-30 17:08:51</td>\n",
       "      <td>2024-08-30 17:08:58</td>\n",
       "      <td>Fehler PET 2 KZE</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>KZE_Strang1 - Warnung Differenzdruck Strang 1 ...</td>\n",
       "      <td>2024-08-30 17:08:50</td>\n",
       "      <td>2024-08-30 17:08:58</td>\n",
       "      <td>Fehler PET 2 KZE</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>KZE_Strang1 - Warnung Differenzdruck Strang 1 ...</td>\n",
       "      <td>2024-08-30 17:08:45</td>\n",
       "      <td>2024-08-30 17:08:58</td>\n",
       "      <td>Fehler PET 2 KZE</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7718</th>\n",
       "      <td>None</td>\n",
       "      <td>Step-032 = Umlauf</td>\n",
       "      <td>2024-08-01 07:58:56</td>\n",
       "      <td>2024-08-01 08:01:27</td>\n",
       "      <td>Erhitzer</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7719</th>\n",
       "      <td>None</td>\n",
       "      <td>Step-001 = Betrieb</td>\n",
       "      <td>2024-08-01 07:58:56</td>\n",
       "      <td>2024-08-01 08:01:27</td>\n",
       "      <td>Rework</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7720</th>\n",
       "      <td>None</td>\n",
       "      <td>Step-003 = Bereit</td>\n",
       "      <td>2024-08-01 07:56:15</td>\n",
       "      <td>2024-08-01 08:35:14</td>\n",
       "      <td>Auslaugfefäß</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7721</th>\n",
       "      <td>None</td>\n",
       "      <td>Step-004 = Betrieb in Reworktank</td>\n",
       "      <td>2024-08-01 07:55:53</td>\n",
       "      <td>2024-08-01 07:56:15</td>\n",
       "      <td>Auslaugfefäß</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7722</th>\n",
       "      <td>None</td>\n",
       "      <td>Step-004 = Dosieren</td>\n",
       "      <td>2024-08-01 07:42:57</td>\n",
       "      <td>2024-08-01 07:58:56</td>\n",
       "      <td>Rework</td>\n",
       "      <td>heating_machine</td>\n",
       "      <td>production</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7719 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     case_id                                           activity  \\\n",
       "1       None                                                CIP   \n",
       "5       None                                            benutzt   \n",
       "6       None  KZE_Strang1 - Warnung Differenzdruck Strang 1 ...   \n",
       "7       None  KZE_Strang1 - Warnung Differenzdruck Strang 1 ...   \n",
       "8       None  KZE_Strang1 - Warnung Differenzdruck Strang 1 ...   \n",
       "...      ...                                                ...   \n",
       "7718    None                                  Step-032 = Umlauf   \n",
       "7719    None                                 Step-001 = Betrieb   \n",
       "7720    None                                  Step-003 = Bereit   \n",
       "7721    None                   Step-004 = Betrieb in Reworktank   \n",
       "7722    None                                Step-004 = Dosieren   \n",
       "\n",
       "         timestamp_start       timestamp_end            object  \\\n",
       "1    2024-08-30 17:44:18 2024-08-30 17:44:18  Status PET 2 KZE   \n",
       "5    2024-08-30 17:08:58 2024-08-30 17:08:58  Status PET 2 KZE   \n",
       "6    2024-08-30 17:08:51 2024-08-30 17:08:58  Fehler PET 2 KZE   \n",
       "7    2024-08-30 17:08:50 2024-08-30 17:08:58  Fehler PET 2 KZE   \n",
       "8    2024-08-30 17:08:45 2024-08-30 17:08:58  Fehler PET 2 KZE   \n",
       "...                  ...                 ...               ...   \n",
       "7718 2024-08-01 07:58:56 2024-08-01 08:01:27          Erhitzer   \n",
       "7719 2024-08-01 07:58:56 2024-08-01 08:01:27            Rework   \n",
       "7720 2024-08-01 07:56:15 2024-08-01 08:35:14      Auslaugfefäß   \n",
       "7721 2024-08-01 07:55:53 2024-08-01 07:56:15      Auslaugfefäß   \n",
       "7722 2024-08-01 07:42:57 2024-08-01 07:58:56            Rework   \n",
       "\n",
       "          object_type higher_level_activity object_attributes  \n",
       "1     heating_machine            production                {}  \n",
       "5     heating_machine            production                {}  \n",
       "6     heating_machine            production                {}  \n",
       "7     heating_machine            production                {}  \n",
       "8     heating_machine            production                {}  \n",
       "...               ...                   ...               ...  \n",
       "7718  heating_machine            production                {}  \n",
       "7719  heating_machine            production                {}  \n",
       "7720  heating_machine            production                {}  \n",
       "7721  heating_machine            production                {}  \n",
       "7722  heating_machine            production                {}  \n",
       "\n",
       "[7719 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Meldungen PET2 KZE 08_2024\n",
    "df = pd.read_excel(files_folder_bronze/\"Aufträge Energie Meldungen/Meldungen PET2 KZE 08_2024.xlsx\", header=2, parse_dates=[\"Aufgetreten\", \"Gegangen\"])\n",
    "print(df.columns)\n",
    "\n",
    "df = df.rename(columns={\n",
    "    \"Aufgetreten\": \"timestamp_start\",\n",
    "    \"Gegangen\": \"timestamp_end\",\n",
    "    \"Objektbezeichnung\": \"object\",\n",
    "    \"Kommende Meldung\": \"activity\",\n",
    "    \"StartTime_Activity\": \"timestamp_start_state\",\n",
    "    \"EndTime_Activity\": \"timestamp_end_state\",\n",
    "})\n",
    "\n",
    "df = df[(df['timestamp_start'] >= time_min) & (df['timestamp_end'] <= time_max)]\n",
    "\n",
    "df['higher_level_activity'] = 'production'\n",
    "df['case_id'] = None\n",
    "df['object_type'] = 'heating_machine'\n",
    "\n",
    "df['object_attributes'] = df.apply(\n",
    "    lambda row: {col: row[col] for col in df.columns if col.startswith('attr')},\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df = df[['case_id', 'activity', 'timestamp_start', 'timestamp_end', 'object',  'object_type', 'higher_level_activity', 'object_attributes']]\n",
    "\n",
    "df_object_2 = df.copy()\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the energy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'temp_Auslauf_EG_(WT2)_5s', 'temp_Einlauf_EG_(WT_2)_5s',\n",
      "       'flow_Kuehlturmwasser_30120FT701_5s', 'flow_Kaltwasser_(WT7)_5s',\n",
      "       'Kuehlturmwassertemp_(WT6)_5s', 'Kaltwassertemp_(WT_7)_5s',\n",
      "       'nach_Kuehler_(WT7)_5s', 'temp_nach_Kuehlturmkuehler_(WT6)_5s',\n",
      "       'Fuellstand_Steriltank_30140LT001_5s',\n",
      "       'Fuellstand_Steriltank_30141LT001_5s', 'flow_Dampf_WT3a/5a)_5s',\n",
      "       'flow_Heisswasser_30120FT721(WT5a)_5s',\n",
      "       'temp_nach_WR2,_vor_Druckerhoehungspumpe_(WT4)_5s',\n",
      "       'temp_nach_Erhitzer_(WT5)_5s', 'temp_nach_WR2_(WT2)_5s',\n",
      "       'temp_nach_Austauscher_2_(WT4)_5s', 'Druck_HW_Anwaermer_(WT3a)_5s',\n",
      "       'temp_HW_Anwaermer_(WT3a)_5s', 'temp_Produkt_Einlauf_30110TT001_1h',\n",
      "       'flow_Vorlaufpumpe_30110FT301_1h', 'temp_vor_Vorwärmer_(WT_2)_1h',\n",
      "       'strom_PERT2_KZE_5s', 'dampf_PET2_KZE_5s', 'strom_gesamt_PERT2_KZE_15m',\n",
      "       'dampf_gesamt_PET2_KZE_15m'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "datetime",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "temp_Auslauf_EG_(WT2)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_Einlauf_EG_(WT_2)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flow_Kuehlturmwasser_30120FT701_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flow_Kaltwasser_(WT7)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Kuehlturmwassertemp_(WT6)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Kaltwassertemp_(WT_7)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nach_Kuehler_(WT7)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_nach_Kuehlturmkuehler_(WT6)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Fuellstand_Steriltank_30140LT001_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Fuellstand_Steriltank_30141LT001_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flow_Dampf_WT3a/5a)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flow_Heisswasser_30120FT721(WT5a)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_nach_WR2,_vor_Druckerhoehungspumpe_(WT4)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_nach_Erhitzer_(WT5)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_nach_WR2_(WT2)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_nach_Austauscher_2_(WT4)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Druck_HW_Anwaermer_(WT3a)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_HW_Anwaermer_(WT3a)_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_Produkt_Einlauf_30110TT001_1h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flow_Vorlaufpumpe_30110FT301_1h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_vor_Vorwärmer_(WT_2)_1h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "strom_PERT2_KZE_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dampf_PET2_KZE_5s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "strom_gesamt_PERT2_KZE_15m",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dampf_gesamt_PET2_KZE_15m",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "604fa221-b157-4f16-a67e-dc680a5037b3",
       "rows": [
        [
         "0",
         "2024-08-01 00:00:00",
         "57.299999872843394",
         "56.70833396911625",
         "31982.25",
         "56431.083333333336",
         "17.56666692",
         "16.408332984999998",
         "15.758333365122484",
         "17.683334032694532",
         "0.0",
         "8723.428629557291",
         "2.0",
         "25524.666666666668",
         "87.19166374206544",
         "92.06666564941406",
         "18.35833279291788",
         "61.43333435058594",
         "2.1966667175293",
         "52.049999235",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.56855805666667",
         "2.0",
         "1440504.53291667",
         "2262.29589870009"
        ],
        [
         "1",
         "2024-08-01 00:01:00",
         "57.2916663487752",
         "56.8166669209798",
         "31956.333333333332",
         "56425.166666666664",
         "17.558333555",
         "16.391666255",
         "15.733333269755034",
         "17.72500038146975",
         "0.0",
         "8731.262044270834",
         "2.0",
         "25325.833333333332",
         "87.20833079020184",
         "92.0583324432373",
         "18.32499933242795",
         "61.54166603088379",
         "2.195833384990695",
         "52.01666641166667",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.40564250916666",
         "2.0",
         null,
         null
        ],
        [
         "2",
         "2024-08-01 00:02:00",
         "57.3500003814697",
         "56.8500003814697",
         "31967.833333333332",
         "56410.75",
         "17.550000190000002",
         "16.374999525",
         "15.699999968210832",
         "17.716667175293",
         "0.0",
         "8793.934407552084",
         "2.0",
         "25487.75",
         "87.2249984741211",
         "92.04166603088379",
         "18.366666158040363",
         "61.52499961853027",
         "2.1933333873748797",
         "51.92500082916666",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.774847030833335",
         "2.0",
         null,
         null
        ],
        [
         "3",
         "2024-08-01 00:03:00",
         "57.325000445048",
         "56.733333587646506",
         "31959.916666666668",
         "56433.916666666664",
         "17.558333555",
         "16.34999943",
         "15.658333381017016",
         "17.7333335876465",
         "0.0",
         "8856.60693359375",
         "2.0",
         "25359.583333333332",
         "87.19999758402508",
         "92.03333282470703",
         "18.291666189829485",
         "61.46666717529297",
         "2.190833389759065",
         "51.90000121166667",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.807797865454546",
         "2.0",
         null,
         null
        ],
        [
         "4",
         "2024-08-01 00:04:00",
         "57.266666412353494",
         "56.725000381469755",
         "31951.25",
         "56390.583333333336",
         "17.56666692",
         "16.31666597",
         "15.641666809717783",
         "17.72500038146975",
         "0.0",
         "8837.021484375",
         "2.0",
         "25538.416666666668",
         "87.19999821980794",
         "91.95000076293945",
         "18.25833320617675",
         "61.441667556762695",
         "2.19000005722046",
         "51.850000384999994",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.42306009916667",
         "2.0",
         null,
         null
        ],
        [
         "5",
         "2024-08-01 00:05:00",
         "57.2583335240682",
         "56.733333587646506",
         "31948.416666666668",
         "56423.083333333336",
         "17.550000190000002",
         "16.299999556666666",
         "15.641666809717783",
         "17.75",
         "0.0",
         "8821.353108723959",
         "2.0",
         "25574.583333333332",
         "87.18333053588869",
         "91.9416675567627",
         "18.2666664123535",
         "61.46666717529297",
         "2.19000005722046",
         "51.83333333666667",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.60836283333333",
         "2.0",
         null,
         null
        ],
        [
         "6",
         "2024-08-01 00:06:00",
         "57.28181838989257",
         "56.71818230368877",
         "31955.909090909092",
         "56431.0",
         "17.581818492727276",
         "16.28181787090909",
         "15.627272952686628",
         "17.727273074063408",
         "0.0",
         "8808.533735795454",
         "2.0",
         "25386.363636363636",
         "87.22727272727273",
         "92.03636308149858",
         "18.299999583851193",
         "61.50909077037465",
         "2.19000005722046",
         "51.75454538545455",
         "13.062797441426573",
         "20968.38227744056",
         "56.84285783330532",
         "37.29068617363637",
         "2.0",
         null,
         null
        ],
        [
         "7",
         "2024-08-01 00:07:00",
         "57.38333415985105",
         "56.74166679382325",
         "31960.666666666668",
         "56423.083333333336",
         "17.533333459999998",
         "16.308332605",
         "15.616666952768933",
         "17.7333335876465",
         "0.0",
         "8774.349283854166",
         "2.0",
         "25343.75",
         "87.18333053588869",
         "92.04999923706055",
         "18.324999491373685",
         "61.50833320617676",
         "2.19000005722046",
         "51.73333358666667",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.07770188666667",
         "2.0",
         null,
         null
        ],
        [
         "8",
         "2024-08-01 00:08:00",
         "57.3083333969116",
         "56.74166679382325",
         "31957.666666666668",
         "56432.5",
         "17.608333745",
         "16.308332605",
         "15.633333524068165",
         "17.74166679382325",
         "0.0",
         "8739.100911458334",
         "2.0",
         "25497.166666666668",
         "87.21666463216145",
         "92.01666641235352",
         "18.30833292007445",
         "61.43333435058594",
         "2.19000005722046",
         "51.666666345833335",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "35.993502299166664",
         "2.0",
         null,
         null
        ],
        [
         "9",
         "2024-08-01 00:09:00",
         "57.28333377838135",
         "56.700000445048055",
         "31969.416666666668",
         "56441.916666666664",
         "17.56666692",
         "16.308332605",
         "15.591667016347225",
         "17.72500038146975",
         "0.0",
         "8739.099039713541",
         "2.0",
         "25370.5",
         "87.21666463216145",
         "91.98333358764648",
         "18.291666348775216",
         "61.46666717529297",
         "2.19000005722046",
         "51.64166609083333",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "35.92283280666667",
         "2.0",
         null,
         null
        ],
        [
         "10",
         "2024-08-01 00:10:00",
         "57.25",
         "56.733333587646506",
         "31969.333333333332",
         "56425.916666666664",
         "17.58333365",
         "16.31666612833333",
         "15.633333524068165",
         "17.7333335876465",
         "0.0",
         "8754.763997395834",
         "2.0",
         "25542.083333333332",
         "87.21666463216145",
         "91.99166679382324",
         "18.32499933242795",
         "61.50833320617676",
         "2.19000005722046",
         "51.63333256666667",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.21123282083334",
         "2.0",
         null,
         null
        ],
        [
         "11",
         "2024-08-01 00:11:00",
         "57.32500012715655",
         "56.7250000635783",
         "31979.333333333332",
         "56437.416666666664",
         "17.58333365",
         "16.308332605",
         "15.616666952768933",
         "17.74166679382325",
         "0.0",
         "8699.928304036459",
         "2.0",
         "25417.583333333332",
         "87.2083314259847",
         "92.01666641235352",
         "18.283332824707",
         "61.5",
         "2.19000005722046",
         "51.57499917083334",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.40043290416667",
         "2.0",
         null,
         null
        ],
        [
         "12",
         "2024-08-01 00:12:00",
         "57.3416668574015",
         "56.733333587646506",
         "31966.416666666668",
         "56418.75",
         "17.608333745",
         "16.324999493333333",
         "15.616666952768933",
         "17.70833396911625",
         "0.0",
         "8692.092936197916",
         "2.0",
         "25379.166666666668",
         "87.1999969482422",
         "92.00833320617676",
         "18.308332602182983",
         "61.50833320617676",
         "2.19000005722046",
         "51.5",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.50364144666667",
         "2.0",
         null,
         null
        ],
        [
         "13",
         "2024-08-01 00:13:00",
         "57.333333651224756",
         "56.716667175293004",
         "31975.0",
         "56425.25",
         "17.591667015000002",
         "16.316666286666663",
         "15.62500023841855",
         "17.683334032694532",
         "0.0",
         "8664.674235026041",
         "2.0",
         "25382.25",
         "87.22499910990398",
         "92.0",
         "18.333332697550432",
         "61.5",
         "2.19000005722046",
         "51.50833320583333",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.38998667333333",
         "2.0",
         null,
         null
        ],
        [
         "14",
         "2024-08-01 00:14:00",
         "57.308332761128696",
         "56.74166679382325",
         "31978.0",
         "56425.333333333336",
         "17.58333365",
         "16.308332605",
         "15.641666809717783",
         "17.70833396911625",
         "0.0",
         "8652.924235026041",
         "2.0",
         "25418.416666666668",
         "87.1999969482422",
         "92.00833320617676",
         "18.308332602182983",
         "61.53333282470703",
         "2.19000005722046",
         "51.50833320583333",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.34531275333333",
         "2.0",
         null,
         null
        ],
        [
         "15",
         "2024-08-01 00:15:00",
         "57.3249998092651",
         "56.716667175293004",
         "31977.916666666668",
         "56462.916666666664",
         "17.591667015000002",
         "16.308332763333333",
         "15.616666952768933",
         "17.691667397817017",
         "0.0",
         "8613.753499348959",
         "2.0",
         "25414.166666666668",
         "87.18333117167155",
         "91.99166679382324",
         "18.316665967305468",
         "61.47500038146973",
         "2.19000005722046",
         "51.44166755916666",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.437849045",
         "2.0",
         "1440513.72111111",
         "2262.73890000094"
        ],
        [
         "16",
         "2024-08-01 00:16:00",
         "57.299999554951945",
         "56.725000381469755",
         "31993.75",
         "56451.25",
         "17.58333365",
         "16.308332763333333",
         "15.616666952768933",
         "17.716667175293",
         "0.0",
         "8594.166341145834",
         "2.0",
         "25461.666666666668",
         "87.19166374206544",
         "91.95833396911621",
         "18.283332824707",
         "61.45000076293945",
         "2.19000005722046",
         "51.40833473583333",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.52314217833333",
         "2.0",
         null,
         null
        ],
        [
         "17",
         "2024-08-01 00:17:00",
         "57.308332761128696",
         "56.75833320617675",
         "31979.333333333332",
         "56439.833333333336",
         "17.625000475",
         "16.31666612833333",
         "15.616666952768933",
         "17.74166679382325",
         "0.0",
         "8578.505045572916",
         "2.0",
         "25288.75",
         "87.20833079020184",
         "92.0",
         "18.333332697550432",
         "61.5",
         "2.19000005722046",
         "51.36666743333333",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.364873886666665",
         "2.0",
         null,
         null
        ],
        [
         "18",
         "2024-08-01 00:18:00",
         "57.318181818181785",
         "56.74545461481268",
         "31973.272727272728",
         "56484.0",
         "17.60000038",
         "16.363635845454546",
         "15.618182095614317",
         "17.736363844438046",
         "0.0",
         "8556.424538352272",
         "2.0",
         "25409.81818181818",
         "87.17272533069958",
         "92.04545385187322",
         "18.336363012140428",
         "61.49090922962535",
         "2.19000005722046",
         "51.363637060909085",
         "13.062797441426573",
         "20968.38227744056",
         "56.84285783330532",
         "36.67148486090909",
         "2.0",
         null,
         null
        ],
        [
         "19",
         "2024-08-01 00:19:00",
         "57.25833320617675",
         "56.716667175293004",
         "31985.25",
         "56475.25",
         "17.56666692",
         "16.333332858333332",
         "15.62500023841855",
         "17.716667175293",
         "0.0",
         "8527.58203125",
         "2.0",
         "25397.25",
         "87.19999758402508",
         "92.04166603088379",
         "18.35833279291788",
         "61.45833396911621",
         "2.19000005722046",
         "51.350000384999994",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.484883945",
         "2.0",
         null,
         null
        ],
        [
         "20",
         "2024-08-01 00:20:00",
         "57.23333326975504",
         "56.74166679382325",
         "31989.416666666668",
         "56450.583333333336",
         "17.558333555",
         "16.341666065",
         "15.616666952768933",
         "17.7333335876465",
         "0.0",
         "8511.90771484375",
         "2.0",
         "25641.916666666668",
         "87.19999758402508",
         "92.04999923706055",
         "18.35833279291788",
         "61.45000076293945",
         "2.19000005722046",
         "51.3249998125",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.408709208333335",
         "2.0",
         null,
         null
        ],
        [
         "21",
         "2024-08-01 00:21:00",
         "57.233333587646506",
         "56.70833396911625",
         "31962.833333333332",
         "56430.333333333336",
         "17.575000285",
         "16.358332795",
         "15.633333524068165",
         "17.72500038146975",
         "0.0",
         "8500.159586588541",
         "2.0",
         "25865.333333333332",
         "87.19166437784831",
         "92.03333282470703",
         "18.566666920979824",
         "61.40000120798746",
         "2.19000005722046",
         "51.34166686083333",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.618342401666666",
         "2.0",
         null,
         null
        ],
        [
         "22",
         "2024-08-01 00:22:00",
         "57.24166679382325",
         "56.725000381469755",
         "31977.916666666668",
         "56434.583333333336",
         "17.58333365",
         "16.34999943",
         "15.616666952768933",
         "17.72500038146975",
         "0.0",
         "8476.661295572916",
         "2.0",
         "25725.666666666668",
         "87.1999969482422",
         "92.03333282470703",
         "18.658333937327082",
         "61.45833396911621",
         "2.19000005722046",
         "51.30833308166666",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.4482917775",
         "2.0",
         null,
         null
        ],
        [
         "23",
         "2024-08-01 00:23:00",
         "57.09166685740152",
         "56.55833371480307",
         "31981.5",
         "56488.25",
         "17.58333365",
         "16.3333327",
         "15.6000003814697",
         "17.774999777475983",
         "0.0",
         "8472.738850911459",
         "2.0",
         "27184.833333333332",
         "87.14999771118164",
         "91.99166679382324",
         "18.633333841959637",
         "61.2583335240682",
         "2.1891667246818547",
         "51.33333333666667",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "38.816441536666666",
         "2.0",
         null,
         null
        ],
        [
         "24",
         "2024-08-01 00:24:00",
         "56.80833371480307",
         "56.30000019073486",
         "31974.333333333332",
         "56489.75",
         "17.61666711",
         "16.374999525",
         "15.650000095367398",
         "17.74166679382325",
         "0.0",
         "8453.157307942709",
         "2.0",
         "26883.916666666668",
         "87.12499809265137",
         "92.0",
         "18.566666920979802",
         "61.12500031789145",
         "2.19000005722046",
         "51.2999995575",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "38.3948904675",
         "2.0",
         null,
         null
        ],
        [
         "25",
         "2024-08-01 00:25:00",
         "57.174999554951995",
         "56.68333276112875",
         "31997.5",
         "56487.5",
         "17.608333745",
         "16.391666255",
         "15.708333174387583",
         "17.75",
         "0.0",
         "8464.901774088541",
         "2.0",
         "26542.5",
         "87.1749979654948",
         "92.00833320617676",
         "18.64166720708212",
         "61.358333587646456",
         "2.19000005722046",
         "51.25",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.68769741083333",
         "2.0",
         null,
         null
        ],
        [
         "26",
         "2024-08-01 00:26:00",
         "56.97500006357828",
         "56.43333371480305",
         "31993.833333333332",
         "56492.75",
         "17.58333365",
         "16.39999962",
         "15.666666666666634",
         "17.7666664123535",
         "0.0",
         "8441.405436197916",
         "2.0",
         "26645.833333333332",
         "87.03333282470703",
         "91.99166679382324",
         "18.48333326975505",
         "61.108332951863616",
         "2.19000005722046",
         "51.25",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.85914103166667",
         "2.0",
         null,
         null
        ],
        [
         "27",
         "2024-08-01 00:27:00",
         "57.00833320617676",
         "56.57499980926516",
         "32003.25",
         "56441.916666666664",
         "17.625000475",
         "16.41666635",
         "15.674999952316249",
         "17.75833320617675",
         "0.0",
         "8445.325846354166",
         "2.0",
         "25682.416666666668",
         "87.21666590372722",
         "92.04999923706055",
         "18.650000572204608",
         "61.350000699361146",
         "2.19000005722046",
         "51.225000062499994",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.54016431083333",
         "2.0",
         null,
         null
        ],
        [
         "28",
         "2024-08-01 00:28:00",
         "56.850000381469755",
         "56.291666030883796",
         "31989.416666666668",
         "56475.916666666664",
         "17.58333365",
         "16.34999943",
         "15.674999952316249",
         "17.75",
         "0.0",
         "8413.986897786459",
         "2.0",
         "26936.666666666668",
         "87.11666488647461",
         "92.04999923706055",
         "18.341666698455807",
         "61.00000031789142",
         "2.1875000596046448",
         "51.149999615000006",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "38.923623721666665",
         "2.0",
         null,
         null
        ],
        [
         "29",
         "2024-08-01 00:29:00",
         "56.70000012715659",
         "56.19999980926516",
         "31991.75",
         "56509.333333333336",
         "17.575000285",
         "16.358332795",
         "15.699999888737965",
         "17.7333335876465",
         "0.0",
         "8413.986897786459",
         "2.0",
         "26486.75",
         "87.22499910990398",
         "92.04166603088379",
         "18.42499971389771",
         "61.050000190734856",
         "2.18666672706604",
         "51.11666551833334",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "38.435299873333335",
         "2.0",
         null,
         null
        ],
        [
         "30",
         "2024-08-01 00:30:00",
         "57.08181693337181",
         "56.59090839732778",
         "31987.363636363636",
         "56463.454545454544",
         "17.54545471818182",
         "16.38181773272727",
         "15.68181809512048",
         "17.736363844438046",
         "0.0",
         "8415.40926846591",
         "2.0",
         "26591.272727272728",
         "87.21818195689808",
         "92.05454462224787",
         "18.618182269009676",
         "61.35454628684303",
         "2.19000005722046",
         "51.11818070454546",
         "13.062797441426573",
         "20968.38227744056",
         "56.84285783330532",
         "38.04867831181818",
         "2.0",
         "1440523.13722222",
         "2262.73890000094"
        ],
        [
         "31",
         "2024-08-01 00:31:00",
         "57.199999809265144",
         "56.69999980926516",
         "31968.75",
         "56437.75",
         "17.550000190000002",
         "16.408332984999998",
         "15.674999952316249",
         "17.75",
         "0.0",
         "8390.486897786459",
         "2.0",
         "25427.75",
         "87.14166450500488",
         "92.02499961853027",
         "18.48333326975505",
         "61.35833454132081",
         "2.19000005722046",
         "51.09166526416667",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.75226662272728",
         "2.0",
         null,
         null
        ],
        [
         "32",
         "2024-08-01 00:32:00",
         "57.38333415985105",
         "56.766666094462046",
         "31960.083333333332",
         "56446.25",
         "17.541666825",
         "16.39999962",
         "15.683333237965867",
         "17.75",
         "0.0",
         "8374.814615885416",
         "2.0",
         "25741.583333333332",
         "87.1999969482422",
         "92.04166603088379",
         "18.466666857401524",
         "61.391667048136405",
         "2.19000005722046",
         "51.074998852499995",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.31317710833333",
         "2.0",
         null,
         null
        ],
        [
         "33",
         "2024-08-01 00:33:00",
         "57.09999910990398",
         "56.49166679382324",
         "31958.5",
         "56436.666666666664",
         "17.58333365",
         "16.3333327",
         "15.683333237965867",
         "17.70833396911625",
         "0.0",
         "8355.233072916666",
         "2.0",
         "25469.666666666668",
         "87.19999758402508",
         "92.04999923706055",
         "18.22500038146975",
         "61.199999491373696",
         "2.1891667246818547",
         "51.08333205833333",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.87992477416667",
         "2.0",
         null,
         null
        ],
        [
         "34",
         "2024-08-01 00:34:00",
         "57.17499987284342",
         "56.62500031789147",
         "31970.083333333332",
         "56432.5",
         "17.558333555",
         "16.358332795",
         "15.674999952316249",
         "17.683334032694532",
         "0.0",
         "8316.066324869791",
         "2.0",
         "25564.416666666668",
         "87.28333346048991",
         "92.01666641235352",
         "18.39999961853029",
         "61.3416668574015",
         "2.19000005722046",
         "51.049999235",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.5379734025",
         "2.0",
         null,
         null
        ],
        [
         "35",
         "2024-08-01 00:35:00",
         "57.29166666666665",
         "56.7416664759318",
         "31978.0",
         "56473.083333333336",
         "17.558333555",
         "16.34999943",
         "15.683333237965867",
         "17.716667175293",
         "0.0",
         "8316.062581380209",
         "2.0",
         "25309.25",
         "87.25833384195964",
         "92.04999923706055",
         "18.433333079020198",
         "61.43333435058594",
         "2.19000005722046",
         "51.05833244083334",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.38665072083334",
         "2.0",
         null,
         null
        ],
        [
         "36",
         "2024-08-01 00:36:00",
         "57.33333333333329",
         "56.7583335240682",
         "31990.333333333332",
         "56434.666666666664",
         "17.558333555",
         "16.34999943",
         "15.6999998092651",
         "17.691667397817017",
         "0.0",
         "8288.645670572916",
         "2.0",
         "25427.666666666668",
         "87.19999758402508",
         "92.06666564941406",
         "18.441666444142673",
         "61.45000076293945",
         "2.19000005722046",
         "51.02499961750001",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.250034015000004",
         "2.0",
         null,
         null
        ],
        [
         "37",
         "2024-08-01 00:37:00",
         "57.3416668574015",
         "56.7416671117147",
         "31967.75",
         "56464.166666666664",
         "17.56666692",
         "16.34999943",
         "15.683333237965867",
         "17.67500066757205",
         "0.0",
         "8261.226725260416",
         "2.0",
         "25430.583333333332",
         "87.19166437784831",
         "92.03333282470703",
         "18.441666444142673",
         "61.47500038146973",
         "2.19000005722046",
         "51.02499961750001",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.220960936666664",
         "2.0",
         null,
         null
        ],
        [
         "38",
         "2024-08-01 00:38:00",
         "57.29166603088375",
         "56.75",
         "31972.083333333332",
         "56427.5",
         "17.550000190000002",
         "16.341666065",
         "15.666666666666634",
         "17.683334032694532",
         "0.0",
         "8222.057942708334",
         "2.0",
         "25465.25",
         "87.18333053588869",
         "92.01666641235352",
         "18.416666348775248",
         "61.483333587646484",
         "2.18833339214325",
         "51.04166602916666",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.15509541916666",
         "2.0",
         null,
         null
        ],
        [
         "39",
         "2024-08-01 00:39:00",
         "57.3500003814697",
         "56.799999872843394",
         "31983.75",
         "56449.833333333336",
         "17.58333365",
         "16.366666159999998",
         "15.683333237965867",
         "17.7000007629395",
         "0.0",
         "8225.974650065104",
         "2.0",
         "25379.916666666668",
         "87.1999969482422",
         "92.02499961853027",
         "18.4666665395101",
         "61.54166603088379",
         "2.1891667246818547",
         "51.01666641166667",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.16166559916667",
         "2.0",
         null,
         null
        ],
        [
         "40",
         "2024-08-01 00:40:00",
         "57.40833473205566",
         "56.808334032694496",
         "31990.916666666668",
         "56447.666666666664",
         "17.550000190000002",
         "16.324999334999998",
         "15.666666666666634",
         "17.683334032694532",
         "0.0",
         "8190.715087890625",
         "2.0",
         "25355.25",
         "87.18333053588869",
         "92.0583324432373",
         "18.41666634877524",
         "61.54166603088379",
         "2.18833339214325",
         "50.97500038249999",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.24303976666666",
         "2.0",
         null,
         null
        ],
        [
         "41",
         "2024-08-01 00:41:00",
         "57.39166768391926",
         "56.83333333333329",
         "31986.5",
         "56501.333333333336",
         "17.575000285",
         "16.358332795",
         "15.708333174387583",
         "17.6833338737488",
         "0.0",
         "8159.38525390625",
         "2.0",
         "25406.083333333332",
         "87.20833079020184",
         "92.04166603088379",
         "18.433333079020198",
         "61.53333282470703",
         "2.1875000596046448",
         "50.950000765",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.71171292454546",
         "2.0",
         null,
         null
        ],
        [
         "42",
         "2024-08-01 00:42:00",
         "57.33636370572174",
         "56.736363844438046",
         "32004.545454545456",
         "56491.90909090909",
         "17.55454566181818",
         "16.35454490181818",
         "15.68181809512048",
         "17.672727931629552",
         "0.0",
         "8150.478471235795",
         "2.0",
         "25462.0",
         "87.2272713401101",
         "92.04545385187322",
         "18.36363584345037",
         "61.50909077037465",
         "2.1854546070098873",
         "50.945455032727274",
         "13.062797441426573",
         "20968.38227744056",
         "56.84285783330532",
         "37.714098843636364",
         "2.0",
         null,
         null
        ],
        [
         "43",
         "2024-08-01 00:43:00",
         "57.3583339055379",
         "56.81666660308835",
         "31992.416666666668",
         "56474.416666666664",
         "17.575000285",
         "16.324999334999998",
         "15.683333237965867",
         "17.716667175293",
         "0.0",
         "8108.466389973958",
         "2.0",
         "25428.416666666668",
         "87.25",
         "92.06666564941406",
         "18.37499952316285",
         "61.56666564941406",
         "2.18666672706604",
         "50.90000121166667",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.44735440454545",
         "2.0",
         null,
         null
        ],
        [
         "44",
         "2024-08-01 00:44:00",
         "57.383334477742494",
         "56.883334477742494",
         "31972.916666666668",
         "56467.916666666664",
         "17.550000190000002",
         "16.358332795",
         "15.658333381017016",
         "17.691667238871283",
         "0.0",
         "8084.958821614583",
         "2.0",
         "25368.416666666668",
         "87.21666463216145",
         "92.04999923706055",
         "18.366666158040363",
         "61.558332443237305",
         "2.18500006198883",
         "50.88333448166666",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.446980795",
         "2.0",
         null,
         null
        ],
        [
         "45",
         "2024-08-01 00:45:00",
         "57.38333415985105",
         "56.8083333969116",
         "31981.5",
         "56463.583333333336",
         "17.56666692",
         "16.324999493333333",
         "15.683333237965867",
         "17.70833396911625",
         "0.0",
         "8092.7939453125",
         "2.0",
         "25386.5",
         "87.1999969482422",
         "92.06666564941406",
         "18.35833279291788",
         "61.53333282470703",
         "2.1800000667572",
         "50.88333448166666",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.53828302916667",
         "2.0",
         "1440532.38722222",
         "2263.59339218844"
        ],
        [
         "46",
         "2024-08-01 00:46:00",
         "57.32500012715655",
         "56.79166603088375",
         "31957.75",
         "56412.166666666664",
         "17.533333459999998",
         "16.324999334999998",
         "15.69166660308835",
         "17.70833396911625",
         "0.0",
         "8041.877278645833",
         "2.0",
         "25447.25",
         "87.1999969482422",
         "92.0583324432373",
         "18.39166625340781",
         "61.52499961853027",
         "2.182500064373015",
         "50.833333654166665",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.78032271166666",
         "2.0",
         null,
         null
        ],
        [
         "47",
         "2024-08-01 00:47:00",
         "57.34166717529295",
         "56.75833320617675",
         "31970.083333333332",
         "56416.666666666664",
         "17.575000285",
         "16.31666597",
         "15.683333237965867",
         "17.691667397817017",
         "0.0",
         "8022.2901611328125",
         "2.0",
         "25429.916666666668",
         "87.20833079020184",
         "92.02499961853027",
         "18.383332888285334",
         "61.5",
         "2.182500064373015",
         "50.8249998125",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "37.48550828416666",
         "2.0",
         null,
         null
        ],
        [
         "48",
         "2024-08-01 00:48:00",
         "57.325000445048",
         "56.74166679382325",
         "31999.666666666668",
         "56447.666666666664",
         "17.558333555",
         "16.31666597",
         "15.6999998092651",
         "17.7000007629395",
         "0.0",
         "8006.623128255208",
         "2.0",
         "25398.916666666668",
         "87.20833079020184",
         "92.03333282470703",
         "18.383332888285334",
         "61.516666412353516",
         "2.18166673183441",
         "50.79999924",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.400483449166664",
         "2.0",
         null,
         null
        ],
        [
         "49",
         "2024-08-01 00:49:00",
         "57.3500003814697",
         "56.7749999364217",
         "31981.5",
         "56477.333333333336",
         "17.558333555",
         "16.31666612833333",
         "15.6999998092651",
         "17.67500066757205",
         "0.0",
         "7983.126383463542",
         "2.0",
         "25373.5",
         "87.1999969482422",
         "92.01666641235352",
         "18.366666158040363",
         "61.516666412353516",
         "2.18166673183441",
         "50.808332764166664",
         "13.062797441426573",
         "20968.38227744056",
         "56.842857833305324",
         "36.11490790166667",
         "2.0",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 26,
        "rows": 44640
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp_Auslauf_EG_(WT2)_5s</th>\n",
       "      <th>temp_Einlauf_EG_(WT_2)_5s</th>\n",
       "      <th>flow_Kuehlturmwasser_30120FT701_5s</th>\n",
       "      <th>flow_Kaltwasser_(WT7)_5s</th>\n",
       "      <th>Kuehlturmwassertemp_(WT6)_5s</th>\n",
       "      <th>Kaltwassertemp_(WT_7)_5s</th>\n",
       "      <th>nach_Kuehler_(WT7)_5s</th>\n",
       "      <th>temp_nach_Kuehlturmkuehler_(WT6)_5s</th>\n",
       "      <th>Fuellstand_Steriltank_30140LT001_5s</th>\n",
       "      <th>...</th>\n",
       "      <th>temp_nach_Austauscher_2_(WT4)_5s</th>\n",
       "      <th>Druck_HW_Anwaermer_(WT3a)_5s</th>\n",
       "      <th>temp_HW_Anwaermer_(WT3a)_5s</th>\n",
       "      <th>temp_Produkt_Einlauf_30110TT001_1h</th>\n",
       "      <th>flow_Vorlaufpumpe_30110FT301_1h</th>\n",
       "      <th>temp_vor_Vorwärmer_(WT_2)_1h</th>\n",
       "      <th>strom_PERT2_KZE_5s</th>\n",
       "      <th>dampf_PET2_KZE_5s</th>\n",
       "      <th>strom_gesamt_PERT2_KZE_15m</th>\n",
       "      <th>dampf_gesamt_PET2_KZE_15m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-08-01 00:00:00</td>\n",
       "      <td>57.300000</td>\n",
       "      <td>56.708334</td>\n",
       "      <td>31982.250000</td>\n",
       "      <td>56431.083333</td>\n",
       "      <td>17.566667</td>\n",
       "      <td>16.408333</td>\n",
       "      <td>15.758333</td>\n",
       "      <td>17.683334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.433334</td>\n",
       "      <td>2.196667</td>\n",
       "      <td>52.049999</td>\n",
       "      <td>13.062797</td>\n",
       "      <td>20968.382277</td>\n",
       "      <td>56.842858</td>\n",
       "      <td>37.568558</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.440505e+06</td>\n",
       "      <td>2262.295899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-01 00:01:00</td>\n",
       "      <td>57.291666</td>\n",
       "      <td>56.816667</td>\n",
       "      <td>31956.333333</td>\n",
       "      <td>56425.166667</td>\n",
       "      <td>17.558334</td>\n",
       "      <td>16.391666</td>\n",
       "      <td>15.733333</td>\n",
       "      <td>17.725000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.541666</td>\n",
       "      <td>2.195833</td>\n",
       "      <td>52.016666</td>\n",
       "      <td>13.062797</td>\n",
       "      <td>20968.382277</td>\n",
       "      <td>56.842858</td>\n",
       "      <td>37.405643</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-01 00:02:00</td>\n",
       "      <td>57.350000</td>\n",
       "      <td>56.850000</td>\n",
       "      <td>31967.833333</td>\n",
       "      <td>56410.750000</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>17.716667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.525000</td>\n",
       "      <td>2.193333</td>\n",
       "      <td>51.925001</td>\n",
       "      <td>13.062797</td>\n",
       "      <td>20968.382277</td>\n",
       "      <td>56.842858</td>\n",
       "      <td>37.774847</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-01 00:03:00</td>\n",
       "      <td>57.325000</td>\n",
       "      <td>56.733334</td>\n",
       "      <td>31959.916667</td>\n",
       "      <td>56433.916667</td>\n",
       "      <td>17.558334</td>\n",
       "      <td>16.349999</td>\n",
       "      <td>15.658333</td>\n",
       "      <td>17.733334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.466667</td>\n",
       "      <td>2.190833</td>\n",
       "      <td>51.900001</td>\n",
       "      <td>13.062797</td>\n",
       "      <td>20968.382277</td>\n",
       "      <td>56.842858</td>\n",
       "      <td>37.807798</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-01 00:04:00</td>\n",
       "      <td>57.266666</td>\n",
       "      <td>56.725000</td>\n",
       "      <td>31951.250000</td>\n",
       "      <td>56390.583333</td>\n",
       "      <td>17.566667</td>\n",
       "      <td>16.316666</td>\n",
       "      <td>15.641667</td>\n",
       "      <td>17.725000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.441668</td>\n",
       "      <td>2.190000</td>\n",
       "      <td>51.850000</td>\n",
       "      <td>13.062797</td>\n",
       "      <td>20968.382277</td>\n",
       "      <td>56.842858</td>\n",
       "      <td>37.423060</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44635</th>\n",
       "      <td>2024-08-31 23:55:00</td>\n",
       "      <td>32.566666</td>\n",
       "      <td>32.683334</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>32.783333</td>\n",
       "      <td>33.408334</td>\n",
       "      <td>32.775000</td>\n",
       "      <td>31.808333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.183334</td>\n",
       "      <td>2.130833</td>\n",
       "      <td>36.691667</td>\n",
       "      <td>33.037872</td>\n",
       "      <td>5.999976</td>\n",
       "      <td>34.367041</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>1.916667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44636</th>\n",
       "      <td>2024-08-31 23:56:00</td>\n",
       "      <td>32.583332</td>\n",
       "      <td>32.666667</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>32.800000</td>\n",
       "      <td>33.391668</td>\n",
       "      <td>32.783333</td>\n",
       "      <td>31.791666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.175000</td>\n",
       "      <td>2.131667</td>\n",
       "      <td>36.683334</td>\n",
       "      <td>33.037872</td>\n",
       "      <td>5.999976</td>\n",
       "      <td>34.367041</td>\n",
       "      <td>0.603444</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44637</th>\n",
       "      <td>2024-08-31 23:57:00</td>\n",
       "      <td>32.558332</td>\n",
       "      <td>32.666666</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>32.816666</td>\n",
       "      <td>33.383334</td>\n",
       "      <td>32.791666</td>\n",
       "      <td>31.824999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.175000</td>\n",
       "      <td>2.130833</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>33.037872</td>\n",
       "      <td>5.999976</td>\n",
       "      <td>34.367041</td>\n",
       "      <td>0.604920</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44638</th>\n",
       "      <td>2024-08-31 23:58:00</td>\n",
       "      <td>32.583332</td>\n",
       "      <td>32.675000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>32.808333</td>\n",
       "      <td>33.400001</td>\n",
       "      <td>32.799999</td>\n",
       "      <td>31.849999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.166667</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>36.683334</td>\n",
       "      <td>33.037872</td>\n",
       "      <td>5.999976</td>\n",
       "      <td>34.367041</td>\n",
       "      <td>0.608587</td>\n",
       "      <td>1.916667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44639</th>\n",
       "      <td>2024-08-31 23:59:00</td>\n",
       "      <td>32.566666</td>\n",
       "      <td>32.677778</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>32.811111</td>\n",
       "      <td>33.388890</td>\n",
       "      <td>32.788888</td>\n",
       "      <td>31.811110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.177778</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>36.688889</td>\n",
       "      <td>33.037872</td>\n",
       "      <td>5.999976</td>\n",
       "      <td>34.367041</td>\n",
       "      <td>0.606114</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44640 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime  temp_Auslauf_EG_(WT2)_5s  \\\n",
       "0     2024-08-01 00:00:00                 57.300000   \n",
       "1     2024-08-01 00:01:00                 57.291666   \n",
       "2     2024-08-01 00:02:00                 57.350000   \n",
       "3     2024-08-01 00:03:00                 57.325000   \n",
       "4     2024-08-01 00:04:00                 57.266666   \n",
       "...                   ...                       ...   \n",
       "44635 2024-08-31 23:55:00                 32.566666   \n",
       "44636 2024-08-31 23:56:00                 32.583332   \n",
       "44637 2024-08-31 23:57:00                 32.558332   \n",
       "44638 2024-08-31 23:58:00                 32.583332   \n",
       "44639 2024-08-31 23:59:00                 32.566666   \n",
       "\n",
       "       temp_Einlauf_EG_(WT_2)_5s  flow_Kuehlturmwasser_30120FT701_5s  \\\n",
       "0                      56.708334                        31982.250000   \n",
       "1                      56.816667                        31956.333333   \n",
       "2                      56.850000                        31967.833333   \n",
       "3                      56.733334                        31959.916667   \n",
       "4                      56.725000                        31951.250000   \n",
       "...                          ...                                 ...   \n",
       "44635                  32.683334                            8.000000   \n",
       "44636                  32.666667                            8.000000   \n",
       "44637                  32.666666                            8.000000   \n",
       "44638                  32.675000                            8.000000   \n",
       "44639                  32.677778                            8.000000   \n",
       "\n",
       "       flow_Kaltwasser_(WT7)_5s  Kuehlturmwassertemp_(WT6)_5s  \\\n",
       "0                  56431.083333                     17.566667   \n",
       "1                  56425.166667                     17.558334   \n",
       "2                  56410.750000                     17.550000   \n",
       "3                  56433.916667                     17.558334   \n",
       "4                  56390.583333                     17.566667   \n",
       "...                         ...                           ...   \n",
       "44635                 17.000000                     32.783333   \n",
       "44636                 17.000000                     32.800000   \n",
       "44637                 17.000000                     32.816666   \n",
       "44638                 17.000000                     32.808333   \n",
       "44639                 17.000000                     32.811111   \n",
       "\n",
       "       Kaltwassertemp_(WT_7)_5s  nach_Kuehler_(WT7)_5s  \\\n",
       "0                     16.408333              15.758333   \n",
       "1                     16.391666              15.733333   \n",
       "2                     16.375000              15.700000   \n",
       "3                     16.349999              15.658333   \n",
       "4                     16.316666              15.641667   \n",
       "...                         ...                    ...   \n",
       "44635                 33.408334              32.775000   \n",
       "44636                 33.391668              32.783333   \n",
       "44637                 33.383334              32.791666   \n",
       "44638                 33.400001              32.799999   \n",
       "44639                 33.388890              32.788888   \n",
       "\n",
       "       temp_nach_Kuehlturmkuehler_(WT6)_5s  \\\n",
       "0                                17.683334   \n",
       "1                                17.725000   \n",
       "2                                17.716667   \n",
       "3                                17.733334   \n",
       "4                                17.725000   \n",
       "...                                    ...   \n",
       "44635                            31.808333   \n",
       "44636                            31.791666   \n",
       "44637                            31.824999   \n",
       "44638                            31.849999   \n",
       "44639                            31.811110   \n",
       "\n",
       "       Fuellstand_Steriltank_30140LT001_5s  ...  \\\n",
       "0                                      0.0  ...   \n",
       "1                                      0.0  ...   \n",
       "2                                      0.0  ...   \n",
       "3                                      0.0  ...   \n",
       "4                                      0.0  ...   \n",
       "...                                    ...  ...   \n",
       "44635                                  0.0  ...   \n",
       "44636                                  0.0  ...   \n",
       "44637                                  0.0  ...   \n",
       "44638                                  0.0  ...   \n",
       "44639                                  0.0  ...   \n",
       "\n",
       "       temp_nach_Austauscher_2_(WT4)_5s  Druck_HW_Anwaermer_(WT3a)_5s  \\\n",
       "0                             61.433334                      2.196667   \n",
       "1                             61.541666                      2.195833   \n",
       "2                             61.525000                      2.193333   \n",
       "3                             61.466667                      2.190833   \n",
       "4                             61.441668                      2.190000   \n",
       "...                                 ...                           ...   \n",
       "44635                         36.183334                      2.130833   \n",
       "44636                         36.175000                      2.131667   \n",
       "44637                         36.175000                      2.130833   \n",
       "44638                         36.166667                      2.130000   \n",
       "44639                         36.177778                      2.130000   \n",
       "\n",
       "       temp_HW_Anwaermer_(WT3a)_5s  temp_Produkt_Einlauf_30110TT001_1h  \\\n",
       "0                        52.049999                           13.062797   \n",
       "1                        52.016666                           13.062797   \n",
       "2                        51.925001                           13.062797   \n",
       "3                        51.900001                           13.062797   \n",
       "4                        51.850000                           13.062797   \n",
       "...                            ...                                 ...   \n",
       "44635                    36.691667                           33.037872   \n",
       "44636                    36.683334                           33.037872   \n",
       "44637                    36.666667                           33.037872   \n",
       "44638                    36.683334                           33.037872   \n",
       "44639                    36.688889                           33.037872   \n",
       "\n",
       "       flow_Vorlaufpumpe_30110FT301_1h  temp_vor_Vorwärmer_(WT_2)_1h  \\\n",
       "0                         20968.382277                     56.842858   \n",
       "1                         20968.382277                     56.842858   \n",
       "2                         20968.382277                     56.842858   \n",
       "3                         20968.382277                     56.842858   \n",
       "4                         20968.382277                     56.842858   \n",
       "...                                ...                           ...   \n",
       "44635                         5.999976                     34.367041   \n",
       "44636                         5.999976                     34.367041   \n",
       "44637                         5.999976                     34.367041   \n",
       "44638                         5.999976                     34.367041   \n",
       "44639                         5.999976                     34.367041   \n",
       "\n",
       "       strom_PERT2_KZE_5s  dampf_PET2_KZE_5s  strom_gesamt_PERT2_KZE_15m  \\\n",
       "0               37.568558           2.000000                1.440505e+06   \n",
       "1               37.405643           2.000000                         NaN   \n",
       "2               37.774847           2.000000                         NaN   \n",
       "3               37.807798           2.000000                         NaN   \n",
       "4               37.423060           2.000000                         NaN   \n",
       "...                   ...                ...                         ...   \n",
       "44635            0.606667           1.916667                         NaN   \n",
       "44636            0.603444           2.000000                         NaN   \n",
       "44637            0.604920           1.833333                         NaN   \n",
       "44638            0.608587           1.916667                         NaN   \n",
       "44639            0.606114           2.000000                         NaN   \n",
       "\n",
       "       dampf_gesamt_PET2_KZE_15m  \n",
       "0                    2262.295899  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "...                          ...  \n",
       "44635                        NaN  \n",
       "44636                        NaN  \n",
       "44637                        NaN  \n",
       "44638                        NaN  \n",
       "44639                        NaN  \n",
       "\n",
       "[44640 rows x 26 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Energie Gesamtwerte\n",
    "# Strom in kWh und Dampf in kg\n",
    "df_energy = pd.read_parquet(files_folder_silver/\"df_sensor_joined.parquet\")\n",
    "print(df_energy.columns)\n",
    "\n",
    "# Assuming 'datetime' is the timestamp column\n",
    "df_energy['datetime'] = pd.to_datetime(df_energy['datetime'])\n",
    "df_energy.set_index('datetime', inplace=True)\n",
    "\n",
    "# Resample by minute and aggregate (e.g., mean for averages, sum for totals - adjust as needed)\n",
    "df_energy = df_energy.resample('min').mean()  # or .sum() if appropriate\n",
    "\n",
    "df_energy.reset_index(inplace=True)\n",
    "\n",
    "df_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([df_object_1, df_object_2], ignore_index=True)\n",
    "\n",
    "df['object'] = df['object'].str.replace('PET 2', '').str.replace('PET', '').str.replace('  ', ' ').str.strip()\n",
    "\n",
    "\n",
    "df_combined = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['material_flow', 'Status KZE', 'Fehler KZE', 'Erhitzer',\n",
       "       'Auslaugfefäß', 'Rework', 'Erhitzer Ausschieben',\n",
       "       'Erhitzer Füllen'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['object'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved successfully to /Users/davidzapata/Documents/GitHub/process_energy_demand_modelling/data/silver/process_4/df_events.parquet\n"
     ]
    }
   ],
   "source": [
    "# Define the output file path\n",
    "output_file_path = files_folder_silver / 'df_events.parquet'\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "df_combined.to_parquet(output_file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved successfully to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_with_higher_level = df_combined[df_combined['higher_level_activity'].notna()].copy()\n",
    "\n",
    "\n",
    "df_higher_level = df_combined[df_combined['higher_level_activity'].isna() & (df_combined['activity'] == 'production')].copy()\n",
    "\n",
    "def find_matching_higher_level(row):\n",
    "\n",
    "    matches = df_higher_level[\n",
    "        (df_higher_level['timestamp_start'] <= row['timestamp_start']) &\n",
    "        (df_higher_level['timestamp_end'] >= row['timestamp_end'])\n",
    "    ]\n",
    "    if not matches.empty:\n",
    "\n",
    "        match = matches.iloc[0]\n",
    "        return match['case_id'], match['object_attributes']\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to each row in df_with_higher_level\n",
    "df_with_higher_level[['matched_case_id', 'matched_object_attributes']] = df_with_higher_level.apply(\n",
    "    lambda row: pd.Series(find_matching_higher_level(row)), axis=1\n",
    ")\n",
    "\n",
    "# Now, update the case_id and merge object_attributes\n",
    "df_with_higher_level['case_id'] = df_with_higher_level['matched_case_id']\n",
    "# For object_attributes, you can merge or update as needed. Assuming you want to add the higher level's attributes\n",
    "df_with_higher_level['object_attributes'] = df_with_higher_level.apply(\n",
    "    lambda row: {**row['object_attributes'], **row['matched_object_attributes']} if row['matched_object_attributes'] else row['object_attributes'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop the temporary columns\n",
    "df_with_higher_level = df_with_higher_level.drop(columns=['matched_case_id', 'matched_object_attributes'])\n",
    "\n",
    "# Now, combine the updated lower-level activities with the unchanged higher-level activities\n",
    "df_final = pd.concat([df_with_higher_level, df_higher_level], ignore_index=True)\n",
    "\n",
    "# Sort by timestamp_start for better order\n",
    "df_final_clean = df_final.sort_values(by='timestamp_start').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Filter for rows where object == 'order'\n",
    "df_orders = df_final_clean[df_final_clean['object'] == 'order'].copy()\n",
    "\n",
    "# Group by case_id and find the maximum timestamp_end for each case_id\n",
    "max_ends = df_orders.groupby('case_id')['timestamp_end'].max()\n",
    "\n",
    "# Prepare a list to hold new rows\n",
    "new_rows = []\n",
    "\n",
    "# For each case_id, create a new activity row\n",
    "for case_id, max_end in max_ends.items():\n",
    "    # Get the original row for this case_id (assuming there's one per case_id for 'order')\n",
    "    original_row = df_orders[df_orders['case_id'] == case_id].iloc[0]\n",
    "    \n",
    "    # Create the new row with the same properties, but new activity and timestamps\n",
    "    new_row = {\n",
    "        'case_id': case_id,\n",
    "        'activity': 'end',  # New activity name; adjust if needed\n",
    "        'timestamp_start': max_end + pd.Timedelta(seconds=1),\n",
    "        'timestamp_end': max_end + pd.Timedelta(seconds=1),  # Ends at the same moment as start (zero duration)\n",
    "        'object': 'order',\n",
    "        'object_type': original_row['object_type'],\n",
    "        'higher_level_activity': original_row['higher_level_activity'],\n",
    "        'object_attributes': original_row['object_attributes']\n",
    "    }\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "# Convert new rows to DataFrame\n",
    "df_new = pd.DataFrame(new_rows)\n",
    "\n",
    "# Append the new rows to df_final_clean\n",
    "df = pd.concat([df_final_clean, df_new], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by timestamp_start for proper ordering\n",
    "df_final_clean = df_final_clean.sort_values(by='timestamp_start').reset_index(drop=True)\n",
    "\n",
    "df_final_clean = df_final_clean[df_final_clean['object_type'].isin(objects_to_analyze)]\n",
    "\n",
    "df_final_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join process and energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_clean\n",
    "\n",
    "df_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_energy.shape)\n",
    "df_energy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def expand_activities_to_timeseries(df_activities, df_energy, energy_cols=None):\n",
    "    \"\"\"\n",
    "    For each activity, filter df_energy to the [timestamp_start, timestamp_end) interval,\n",
    "    then create rows combining activity info with each energy timestamp's data.\n",
    "    Append all into one DataFrame, preserving the complete time series per activity.\n",
    "    \"\"\"\n",
    "    if energy_cols is None:\n",
    "        energy_cols = [col for col in df_energy.columns if col != 'datetime']\n",
    "    \n",
    "    expanded_rows = []\n",
    "    \n",
    "    for _, activity_row in df_activities.iterrows():\n",
    "        start = activity_row['timestamp_start_log']\n",
    "        end = activity_row['timestamp_end_log']\n",
    "        \n",
    "        # Filter energy data within the activity's time interval\n",
    "        mask = (df_energy['datetime_energy'] >= start) & (df_energy['datetime_energy'] < end)\n",
    "        energy_subset = df_energy.loc[mask, ['datetime_energy'] + energy_cols]\n",
    "        \n",
    "        # Create new rows: activity data + each energy row\n",
    "        for _, energy_row in energy_subset.iterrows():\n",
    "            new_row = activity_row.to_dict()\n",
    "            new_row.update(energy_row.to_dict())  # Merge energy data\n",
    "            expanded_rows.append(new_row)\n",
    "    \n",
    "    return pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Rename columns in df_final_clean to add suffix '_log'\n",
    "df_activites = df_final_clean.add_suffix('_log')\n",
    "\n",
    "# Rename columns in df_energy to add prefix '_sen', except 'datetime'\n",
    "df_energys = df_energy.add_suffix('_energy')\n",
    "\n",
    "# Usage: Use all sensor columns\n",
    "df_expanded = expand_activities_to_timeseries(df_activites, df_energys)\n",
    "\n",
    "# View result\n",
    "print(df_expanded.head())\n",
    "print(f\"Shape: {df_expanded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_expanded is already created from the previous code\n",
    "\n",
    "# Group by datetime_energy and collect activities\n",
    "overlaps = df_expanded.groupby('datetime_energy')['activity_log'].apply(list).reset_index()\n",
    "\n",
    "# Add a column for the number of activities\n",
    "overlaps['num_activities'] = overlaps['activity_log'].apply(len)\n",
    "\n",
    "# # Filter for overlaps (more than one activity)\n",
    "# overlapping = overlaps[overlaps['num_activities'] > 1]\n",
    "\n",
    "overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sim_extractor import extract_process\n",
    "\n",
    "df = df_final_clean.copy()\n",
    "\n",
    "activity_stats_df = extract_process(df)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROBABILISTIC END ACTIVITY STATS:\")\n",
    "print(\"=\"*50)\n",
    "display(activity_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a produciton plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_final_clean.copy()\n",
    "\n",
    "df = df[df['object_type'].isin(objects_to_analyze)]\n",
    "\n",
    "df = df[['case_id', 'activity', 'timestamp_start', 'timestamp_end', 'object_attributes']]\n",
    "\n",
    "df = df.dropna(subset=['case_id'])\n",
    "\n",
    "df = df.drop_duplicates(subset=['case_id'])\n",
    "\n",
    "production_plan = df.copy()\n",
    "\n",
    "production_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation import ProcessSimulation\n",
    "import random\n",
    "\n",
    "# Drop rows where case_id is NaN (or missing)\n",
    "df_compare = df_final_clean.dropna(subset=['case_id'])\n",
    "\n",
    "simulated_log = ProcessSimulation(activity_stats_df, production_plan).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comapre the results of the sim with the real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create temporary files for the visualizations\n",
    "with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_real, \\\n",
    "     tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_sim:\n",
    "    \n",
    "    temp_real_path = temp_real.name\n",
    "    temp_sim_path = temp_sim.name\n",
    "\n",
    "try:\n",
    "    # Generate and save real data heuristic net\n",
    "    df_real = df_compare.copy()\n",
    "    event_log_real = pm4py.format_dataframe(df_real, case_id='case_id', activity_key='activity', timestamp_key='timestamp_start')\n",
    "    net_real = pm4py.discover_heuristics_net(event_log_real)\n",
    "    pm4py.save_vis_heuristics_net(net_real, temp_real_path, bgcolor='white', dpi=500)\n",
    "    \n",
    "    # Generate and save simulated data heuristic net\n",
    "    df_sim = simulated_log.copy()\n",
    "    event_log_sim = pm4py.format_dataframe(df_sim, case_id='case_id', activity_key='activity', timestamp_key='timestamp_start')\n",
    "    net_sim = pm4py.discover_heuristics_net(event_log_sim)\n",
    "    pm4py.save_vis_heuristics_net(net_sim, temp_sim_path, bgcolor='white', dpi=500)\n",
    "    \n",
    "    # Load and display side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    # Real data on the left\n",
    "    img_real = mpimg.imread(temp_real_path)\n",
    "    ax1.imshow(img_real)\n",
    "    ax1.set_title('Real Data Process Model', fontsize=16, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Simulated data on the right\n",
    "    img_sim = mpimg.imread(temp_sim_path)\n",
    "    ax2.imshow(img_sim)\n",
    "    ax2.set_title('Simulated Data Process Model', fontsize=16, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "finally:\n",
    "    # Clean up temporary files\n",
    "    if os.path.exists(temp_real_path):\n",
    "        os.unlink(temp_real_path)\n",
    "    if os.path.exists(temp_sim_path):\n",
    "        os.unlink(temp_sim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the simulated log first\n",
    "print(\"Simulated log columns:\", simulated_log.columns.tolist())\n",
    "print(\"Simulated log shape:\", simulated_log.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(simulated_log.head())\n",
    "print(\"\\nActivity column type:\", type(simulated_log['activity'].iloc[0]))\n",
    "print(\"Sample activities:\")\n",
    "print(simulated_log['activity'].value_counts().head(10))\n",
    "\n",
    "# Check for any problematic data\n",
    "print(\"\\nAny null values in key columns:\")\n",
    "print(\"case_id nulls:\", simulated_log['case_id'].isnull().sum())\n",
    "print(\"activity nulls:\", simulated_log['activity'].isnull().sum()) \n",
    "print(\"timestamp_start nulls:\", simulated_log['timestamp_start'].isnull().sum())\n",
    "\n",
    "# Clean the data before creating the process map\n",
    "df_clean = simulated_log.copy()\n",
    "\n",
    "# Ensure proper data types\n",
    "df_clean['case_id'] = df_clean['case_id'].astype(str)\n",
    "df_clean['activity'] = df_clean['activity'].astype(str)\n",
    "df_clean['timestamp_start'] = pd.to_datetime(df_clean['timestamp_start'])\n",
    "\n",
    "# Remove any rows with missing essential data\n",
    "df_clean = df_clean.dropna(subset=['case_id', 'activity', 'timestamp_start'])\n",
    "\n",
    "print(f\"\\nCleaned data shape: {df_clean.shape}\")\n",
    "print(\"Sample activities after cleaning:\")\n",
    "print(df_clean['activity'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from collections import Counter\n",
    "import pm4py\n",
    "\n",
    "def comprehensive_simulation_evaluation(simulated_df, real_df, case_col='case_id', \n",
    "                                       activity_col='activity', start_col='timestamp_start', \n",
    "                                       end_col='timestamp_end'):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of simulation quality based on process mining literature\n",
    "    \n",
    "    Metrics based on:\n",
    "    - Rozinat et al. (2008): Conformance checking of processes based on monitoring real behavior\n",
    "    - van der Aalst et al. (2010): Process mining manifesto\n",
    "    - Burattin & Sperduti (2010): Automatic determination of parameters' values for heuristics miner++\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE SIMULATION EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ========== 1. BASIC PROCESS METRICS ==========\n",
    "    print(\"\\n1. BASIC PROCESS METRICS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Event counts\n",
    "    sim_events = len(simulated_df)\n",
    "    real_events = len(real_df)\n",
    "    event_ratio = sim_events / real_events if real_events > 0 else 0\n",
    "    \n",
    "    # Case counts  \n",
    "    sim_cases = simulated_df[case_col].nunique()\n",
    "    real_cases = real_df[case_col].nunique()\n",
    "    case_ratio = sim_cases / real_cases if real_cases > 0 else 0\n",
    "    \n",
    "    print(f\"Events - Real: {real_events}, Sim: {sim_events}, Ratio: {event_ratio:.3f}\")\n",
    "    print(f\"Cases - Real: {real_cases}, Sim: {sim_cases}, Ratio: {case_ratio:.3f}\")\n",
    "    \n",
    "    results['basic_metrics'] = {\n",
    "        'event_count_ratio': event_ratio,\n",
    "        'case_count_ratio': case_ratio,\n",
    "        'event_count_error': abs(1 - event_ratio),\n",
    "        'case_count_error': abs(1 - case_ratio)\n",
    "    }\n",
    "    \n",
    "    # ========== 2. ACTIVITY FREQUENCY ANALYSIS ==========\n",
    "    print(\"\\n2. ACTIVITY FREQUENCY ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Activity frequencies\n",
    "    sim_activity_freq = simulated_df[activity_col].value_counts(normalize=True).sort_index()\n",
    "    real_activity_freq = real_df[activity_col].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    # Align activities (handle missing activities)\n",
    "    all_activities = set(sim_activity_freq.index) | set(real_activity_freq.index)\n",
    "    sim_freq_aligned = pd.Series([sim_activity_freq.get(act, 0) for act in all_activities], index=all_activities)\n",
    "    real_freq_aligned = pd.Series([real_activity_freq.get(act, 0) for act in all_activities], index=all_activities)\n",
    "    \n",
    "    # Jensen-Shannon divergence for activity distributions\n",
    "    js_divergence = jensenshannon(sim_freq_aligned.values, real_freq_aligned.values)\n",
    "    \n",
    "    # Mean Absolute Error of frequencies\n",
    "    freq_mae = np.mean(np.abs(sim_freq_aligned.values - real_freq_aligned.values))\n",
    "    \n",
    "    print(f\"Activity Coverage - Real: {len(real_activity_freq)}, Sim: {len(sim_activity_freq)}\")\n",
    "    print(f\"Jensen-Shannon Divergence (activities): {js_divergence:.4f} (0=perfect, 1=worst)\")\n",
    "    print(f\"Mean Absolute Error (frequencies): {freq_mae:.4f}\")\n",
    "    \n",
    "    results['activity_metrics'] = {\n",
    "        'js_divergence': js_divergence,\n",
    "        'frequency_mae': freq_mae,\n",
    "        'activity_coverage_ratio': len(sim_activity_freq) / len(real_activity_freq) if len(real_activity_freq) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    # ========== 3. DURATION ANALYSIS ==========\n",
    "    print(\"\\n3. DURATION ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Calculate durations\n",
    "    sim_durations = (simulated_df[end_col] - simulated_df[start_col]).dt.total_seconds() / 60\n",
    "    real_durations = (real_df[end_col] - real_df[start_col]).dt.total_seconds() / 60\n",
    "    \n",
    "    # Statistical comparison\n",
    "    duration_ks_stat, duration_ks_pvalue = stats.ks_2samp(sim_durations, real_durations)\n",
    "    \n",
    "    # Duration statistics comparison\n",
    "    duration_stats = pd.DataFrame({\n",
    "        'Real': [real_durations.mean(), real_durations.median(), real_durations.std()],\n",
    "        'Simulated': [sim_durations.mean(), sim_durations.median(), sim_durations.std()],\n",
    "    }, index=['Mean', 'Median', 'Std'])\n",
    "    \n",
    "    duration_stats['Error'] = np.abs(duration_stats['Simulated'] - duration_stats['Real']) / duration_stats['Real']\n",
    "    \n",
    "    print(\"Duration Statistics:\")\n",
    "    print(duration_stats.round(3))\n",
    "    print(f\"\\nKolmogorov-Smirnov Test: KS={duration_ks_stat:.4f}, p-value={duration_ks_pvalue:.4f}\")\n",
    "    print(f\"(p > 0.05 suggests distributions are similar)\")\n",
    "    \n",
    "    results['duration_metrics'] = {\n",
    "        'ks_statistic': duration_ks_stat,\n",
    "        'ks_pvalue': duration_ks_pvalue,\n",
    "        'mean_duration_error': duration_stats.loc['Mean', 'Error'],\n",
    "        'median_duration_error': duration_stats.loc['Median', 'Error'],\n",
    "        'std_duration_error': duration_stats.loc['Std', 'Error']\n",
    "    }\n",
    "    \n",
    "    # ========== 4. CASE-LEVEL ANALYSIS ==========\n",
    "    print(\"\\n4. CASE-LEVEL ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Events per case\n",
    "    sim_events_per_case = simulated_df.groupby(case_col).size()\n",
    "    real_events_per_case = real_df.groupby(case_col).size()\n",
    "    \n",
    "    # Statistical test for events per case\n",
    "    events_per_case_ks, events_per_case_pvalue = stats.ks_2samp(sim_events_per_case, real_events_per_case)\n",
    "    \n",
    "    case_stats = pd.DataFrame({\n",
    "        'Real': [real_events_per_case.mean(), real_events_per_case.median(), real_events_per_case.std()],\n",
    "        'Simulated': [sim_events_per_case.mean(), sim_events_per_case.median(), sim_events_per_case.std()],\n",
    "    }, index=['Mean', 'Median', 'Std'])\n",
    "    \n",
    "    case_stats['Error'] = np.abs(case_stats['Simulated'] - case_stats['Real']) / case_stats['Real']\n",
    "    \n",
    "    print(\"Events per Case Statistics:\")\n",
    "    print(case_stats.round(3))\n",
    "    print(f\"\\nKS Test (events per case): KS={events_per_case_ks:.4f}, p-value={events_per_case_pvalue:.4f}\")\n",
    "    \n",
    "    results['case_metrics'] = {\n",
    "        'events_per_case_ks': events_per_case_ks,\n",
    "        'events_per_case_pvalue': events_per_case_pvalue,\n",
    "        'mean_events_per_case_error': case_stats.loc['Mean', 'Error'],\n",
    "        'median_events_per_case_error': case_stats.loc['Median', 'Error']\n",
    "    }\n",
    "    \n",
    "    # ========== 5. CONTROL-FLOW ANALYSIS ==========\n",
    "    print(\"\\n5. CONTROL-FLOW ANALYSIS (Directly-Follows Graph)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create event logs for pm4py\n",
    "    sim_log = pm4py.format_dataframe(simulated_df, case_id=case_col, activity_key=activity_col, timestamp_key=start_col)\n",
    "    real_log = pm4py.format_dataframe(real_df, case_id=case_col, activity_key=activity_col, timestamp_key=start_col)\n",
    "    \n",
    "    # Discover DFGs\n",
    "    sim_dfg, sim_start, sim_end = pm4py.discover_dfg(sim_log)\n",
    "    real_dfg, real_start, real_end = pm4py.discover_dfg(real_log)\n",
    "    \n",
    "    # Compare DFG edges\n",
    "    sim_edges = set(sim_dfg.keys())\n",
    "    real_edges = set(real_dfg.keys())\n",
    "    \n",
    "    edge_precision = len(sim_edges & real_edges) / len(sim_edges) if len(sim_edges) > 0 else 0\n",
    "    edge_recall = len(sim_edges & real_edges) / len(real_edges) if len(real_edges) > 0 else 0\n",
    "    edge_f1 = 2 * (edge_precision * edge_recall) / (edge_precision + edge_recall) if (edge_precision + edge_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"DFG Edges - Real: {len(real_edges)}, Sim: {len(sim_edges)}, Common: {len(sim_edges & real_edges)}\")\n",
    "    print(f\"Edge Precision: {edge_precision:.4f}\")\n",
    "    print(f\"Edge Recall: {edge_recall:.4f}\")\n",
    "    print(f\"Edge F1-Score: {edge_f1:.4f}\")\n",
    "    \n",
    "    # Compare start/end activities (convert dict keys to sets)\n",
    "    sim_start_set = set(sim_start.keys())\n",
    "    real_start_set = set(real_start.keys())\n",
    "    sim_end_set = set(sim_end.keys())\n",
    "    real_end_set = set(real_end.keys())\n",
    "    \n",
    "    start_jaccard = len(sim_start_set & real_start_set) / len(sim_start_set | real_start_set) if len(sim_start_set | real_start_set) > 0 else 0\n",
    "    end_jaccard = len(sim_end_set & real_end_set) / len(sim_end_set | real_end_set) if len(sim_end_set | real_end_set) > 0 else 0\n",
    "    \n",
    "    print(f\"Start Activities Jaccard: {start_jaccard:.4f}\")\n",
    "    print(f\"End Activities Jaccard: {end_jaccard:.4f}\")\n",
    "    \n",
    "    results['control_flow_metrics'] = {\n",
    "        'edge_precision': edge_precision,\n",
    "        'edge_recall': edge_recall,\n",
    "        'edge_f1_score': edge_f1,\n",
    "        'start_activities_jaccard': start_jaccard,\n",
    "        'end_activities_jaccard': end_jaccard\n",
    "    }\n",
    "    \n",
    "    # ========== 6. OVERALL QUALITY SCORE ==========\n",
    "    print(\"\\n6. OVERALL QUALITY ASSESSMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Weighted quality score (based on literature importance)\n",
    "    quality_components = {\n",
    "        'Event Count': (1 - results['basic_metrics']['event_count_error'], 0.15),\n",
    "        'Activity Distribution': (1 - results['activity_metrics']['js_divergence'], 0.25),\n",
    "        'Duration Distribution': (1 - results['duration_metrics']['ks_statistic'], 0.20),\n",
    "        'Case Structure': (1 - results['case_metrics']['events_per_case_ks'], 0.15),\n",
    "        'Control Flow': (results['control_flow_metrics']['edge_f1_score'], 0.25)\n",
    "    }\n",
    "    \n",
    "    overall_score = sum(score * weight for (score, weight) in quality_components.values())\n",
    "    \n",
    "    print(\"Quality Components:\")\n",
    "    for component, (score, weight) in quality_components.items():\n",
    "        print(f\"  {component:20}: {score:.4f} (weight: {weight:.2f})\")\n",
    "    print(f\"\\nOVERALL QUALITY SCORE: {overall_score:.4f} (0=worst, 1=perfect)\")\n",
    "    \n",
    "    if overall_score >= 0.8:\n",
    "        quality_assessment = \"EXCELLENT\"\n",
    "    elif overall_score >= 0.6:\n",
    "        quality_assessment = \"GOOD\"\n",
    "    elif overall_score >= 0.4:\n",
    "        quality_assessment = \"FAIR\"\n",
    "    else:\n",
    "        quality_assessment = \"POOR\"\n",
    "    \n",
    "    print(f\"QUALITY ASSESSMENT: {quality_assessment}\")\n",
    "    \n",
    "    results['overall_score'] = overall_score\n",
    "    results['quality_assessment'] = quality_assessment\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simulation_comparison(simulated_df, real_df, case_col='case_id', \n",
    "                             activity_col='activity', start_col='timestamp_start', \n",
    "                             end_col='timestamp_end'):\n",
    "    \"\"\"Create detailed comparison plots\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Simulation vs Real Data Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Activity frequencies\n",
    "    sim_freq = simulated_df[activity_col].value_counts()\n",
    "    real_freq = real_df[activity_col].value_counts()\n",
    "    \n",
    "    all_activities = list(set(sim_freq.index) | set(real_freq.index))\n",
    "    sim_aligned = [sim_freq.get(act, 0) for act in all_activities]\n",
    "    real_aligned = [real_freq.get(act, 0) for act in all_activities]\n",
    "    \n",
    "    x = np.arange(len(all_activities))\n",
    "    axes[0,0].bar(x - 0.2, real_aligned, 0.4, label='Real', alpha=0.7, color='skyblue')\n",
    "    axes[0,0].bar(x + 0.2, sim_aligned, 0.4, label='Simulated', alpha=0.7, color='orange')\n",
    "    axes[0,0].set_title('Activity Frequencies')\n",
    "    axes[0,0].set_xlabel('Activities')\n",
    "    axes[0,0].set_ylabel('Count')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Duration distributions\n",
    "    sim_durations = (simulated_df[end_col] - simulated_df[start_col]).dt.total_seconds() / 60\n",
    "    real_durations = (real_df[end_col] - real_df[start_col]).dt.total_seconds() / 60\n",
    "    \n",
    "    axes[0,1].hist(real_durations, alpha=0.7, label='Real', bins=30, density=True, color='skyblue')\n",
    "    axes[0,1].hist(sim_durations, alpha=0.7, label='Simulated', bins=30, density=True, color='orange')\n",
    "    axes[0,1].set_title('Activity Duration Distributions')\n",
    "    axes[0,1].set_xlabel('Duration (minutes)')\n",
    "    axes[0,1].set_ylabel('Density')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # 3. Events per case\n",
    "    sim_events_per_case = simulated_df.groupby(case_col).size()\n",
    "    real_events_per_case = real_df.groupby(case_col).size()\n",
    "    \n",
    "    axes[1,0].hist(real_events_per_case, alpha=0.7, label='Real', bins=20, density=True, color='skyblue')\n",
    "    axes[1,0].hist(sim_events_per_case, alpha=0.7, label='Simulated', bins=20, density=True, color='orange')\n",
    "    axes[1,0].set_title('Events per Case Distribution')\n",
    "    axes[1,0].set_xlabel('Events per Case')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # 4. Cumulative case duration\n",
    "    sim_case_durations = simulated_df.groupby(case_col).apply(\n",
    "        lambda x: (x[end_col].max() - x[start_col].min()).total_seconds() / 3600\n",
    "    )\n",
    "    real_case_durations = real_df.groupby(case_col).apply(\n",
    "        lambda x: (x[end_col].max() - x[start_col].min()).total_seconds() / 3600\n",
    "    )\n",
    "    \n",
    "    axes[1,1].hist(real_case_durations, alpha=0.7, label='Real', bins=20, density=True, color='skyblue')\n",
    "    axes[1,1].hist(sim_case_durations, alpha=0.7, label='Simulated', bins=20, density=True, color='orange')\n",
    "    axes[1,1].set_title('Case Duration Distribution')\n",
    "    axes[1,1].set_xlabel('Case Duration (hours)')\n",
    "    axes[1,1].set_ylabel('Density')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the comprehensive evaluation\n",
    "print(\"🔍 RUNNING COMPREHENSIVE SIMULATION EVALUATION\")\n",
    "print(\"Based on process mining literature (Rozinat et al., van der Aalst et al., Burattin & Sperduti)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluation_results = comprehensive_simulation_evaluation(simulated_log, df_final_clean)\n",
    "\n",
    "print(\"\\n📊 GENERATING COMPARISON PLOTS\")\n",
    "plot_simulation_comparison(simulated_log, df_final_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def expand_activities_to_timeseries(df_activities, df_energy, energy_cols=None):\n",
    "    \"\"\"\n",
    "    For each activity, filter df_energy to the [timestamp_start, timestamp_end) interval,\n",
    "    then create rows combining activity info with each energy timestamp's data.\n",
    "    Append all into one DataFrame, preserving the complete time series per activity.\n",
    "    \"\"\"\n",
    "    if energy_cols is None:\n",
    "        energy_cols = [col for col in df_energy.columns if col != 'datetime']\n",
    "    \n",
    "    expanded_rows = []\n",
    "    \n",
    "    for _, activity_row in df_activities.iterrows():\n",
    "        start = activity_row['timestamp_start_log']\n",
    "        end = activity_row['timestamp_end_log']\n",
    "        \n",
    "        # Filter energy data within the activity's time interval\n",
    "        mask = (df_energy['datetime_energy'] >= start) & (df_energy['datetime_energy'] < end)\n",
    "        energy_subset = df_energy.loc[mask, ['datetime_energy'] + energy_cols]\n",
    "        \n",
    "        # Create new rows: activity data + each energy row\n",
    "        for _, energy_row in energy_subset.iterrows():\n",
    "            new_row = activity_row.to_dict()\n",
    "            new_row.update(energy_row.to_dict())  # Merge energy data\n",
    "            expanded_rows.append(new_row)\n",
    "    \n",
    "    return pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Rename columns in df_final_clean to add suffix '_log'\n",
    "df_activites = df_final_clean.add_suffix('_log')\n",
    "\n",
    "# Rename columns in df_energy to add prefix '_sen', except 'datetime'\n",
    "df_energys = df_energy.add_suffix('_energy')\n",
    "\n",
    "# Usage: Use all sensor columns\n",
    "df_expanded = expand_activities_to_timeseries(df_activites, df_energys)\n",
    "\n",
    "# View result\n",
    "print(df_expanded.head())\n",
    "print(f\"Shape: {df_expanded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_expanded is already created from the previous code\n",
    "\n",
    "# Group by datetime_energy and collect activities\n",
    "overlaps = df_expanded.groupby('datetime_energy')['activity_log'].apply(list).reset_index()\n",
    "\n",
    "# Add a column for the number of activities\n",
    "overlaps['num_activities'] = overlaps['activity_log'].apply(len)\n",
    "\n",
    "# # Filter for overlaps (more than one activity)\n",
    "# overlapping = overlaps[overlaps['num_activities'] > 1]\n",
    "\n",
    "overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded['activity_log'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Filter and prepare the data from df_expanded\n",
    "df_plot = df_expanded.copy()\n",
    "df_plot = df_plot[df_plot['activity_log'].isin(['Step-032 = Umlauf', 'Step-030 = Produktion'])]  # Adjust activity name if needed based on df_expanded['activity_log'].unique()\n",
    "df_plot['timestamp_start'] = pd.to_datetime(df_plot['timestamp_start_log'])  # Use the correct column name from df_expanded\n",
    "df_plot = df_plot[df_plot['timestamp_start'].dt.date == pd.to_datetime('2024-08-14').date()]  # Adjust date if needed\n",
    "#df_plot = df_plot.dropna()    \n",
    "df_plot['type'] = 'actual'\n",
    "df_plot['activity_type'] = df_plot['activity_log'] + '_actual'  # Use activity_log for consistency\n",
    "\n",
    "# Plotting with Plotly for one energy feature (e.g., heating_power_energy)\n",
    "fig = px.line(\n",
    "    df_plot,\n",
    "    x='datetime_energy',\n",
    "    y='flow_Heisswasser_30120FT721(WT5a)_5s_energy',  # Changed to an energy feature column from df_expanded\n",
    "    title='Heating Power Energy Over Time',\n",
    "    labels={'activity_log'},\n",
    "    line_shape='linear',\n",
    "    color='activity_type'\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    xaxis_title='Timestamp Start',\n",
    "    yaxis_title='Heating Power Energy (kW)',\n",
    "    title_font_size=16,\n",
    "    xaxis=dict(tickangle=45),  # Rotate x-axis labels\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_energy.copy()\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])  # Use the correct column name from df_expanded\n",
    "df = df[df['datetime'].dt.date == pd.to_datetime('2024-08-14').date()]  # Adjust date if needed\n",
    "\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x='datetime',\n",
    "    y='flow_Heisswasser_30120FT721(WT5a)_5s',  # Changed to an energy feature column from df_expanded\n",
    "    title='Heating Power Energy Over Time',\n",
    "    labels={'activity_log'},\n",
    "    line_shape='linear',\n",
    "    #color='activity_type'\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    xaxis_title='Timestamp Start',\n",
    "    yaxis_title='Heating Power Energy (kW)',\n",
    "    title_font_size=16,\n",
    "    xaxis=dict(tickangle=45),  # Rotate x-axis labels\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Identify columns ending with \"_energy\"\n",
    "energy_cols = [col for col in df_expanded.columns if col.endswith('_energy')]\n",
    "\n",
    "# Set up subplots: calculate rows and columns for grid\n",
    "n_cols = 3  # Adjust number of columns per row as needed\n",
    "n_rows = (len(energy_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(energy_cols):\n",
    "    sns.lineplot(data=df_expanded, x='datetime_energy', y=col, ax=axes[i])\n",
    "    axes[i].set_title(f'{col}')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = df_activites.copy()\n",
    "\n",
    "\n",
    "df['activity_log'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_expanded.copy()\n",
    "\n",
    "# Count unique timestamp_start_log per activity_log (i.e., unique activity instances per activity type)\n",
    "unique_instances_per_activity = df.groupby('activity_log')['timestamp_start_log'].nunique()\n",
    "\n",
    "print(\"Unique activity instances per activity type:\")\n",
    "print(unique_instances_per_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define activities\n",
    "activities = ['Step-032 = Umlauf', 'Step-030 = Produktion']\n",
    "\n",
    "# Get all energy columns (excluding datetime_energy)\n",
    "energy_cols = [col for col in df_expanded.columns if col.endswith('_energy') and col != 'datetime_energy']\n",
    "\n",
    "print(f\"Activities: {activities}\")\n",
    "print(f\"Energy variables: {energy_cols}\")\n",
    "\n",
    "for activity in activities:\n",
    "    print(f\"\\nProcessing activity: {activity}\")\n",
    "    \n",
    "    # Filter for this activity\n",
    "    df_act = df_expanded[df_expanded['activity_log'] == activity].copy()\n",
    "    \n",
    "    # Create activity_instance_id\n",
    "    df_act['activity_instance_id'] = df_act.groupby(['case_id_log', 'activity_log', 'timestamp_start_log']).ngroup()\n",
    "    \n",
    "    # Calculate total number of instances for this activity\n",
    "    total_instances = df_act['activity_instance_id'].nunique()\n",
    "    print(f\"Total number of instances for {activity}: {total_instances}\")\n",
    "    \n",
    "    # Calculate number of subplots\n",
    "    n_vars = len(energy_cols)\n",
    "    n_cols = 3  # Adjust columns per row\n",
    "    n_rows = (n_vars + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    # Ensure axes is always a flat array\n",
    "    axes = np.array(axes).flatten()\n",
    "    \n",
    "    for i, variable in enumerate(energy_cols):\n",
    "        if i >= len(axes):\n",
    "            break  # Safety check\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Count instances with data for this variable\n",
    "        instances_with_data = 0\n",
    "        for instance_id in df_act['activity_instance_id'].unique():\n",
    "            instance_data = df_act[df_act['activity_instance_id'] == instance_id]\n",
    "            if instance_data[variable].notna().any():  # At least one non-NaN value\n",
    "                instances_with_data += 1\n",
    "        \n",
    "        print(f\"  {variable}: {instances_with_data} instances have data (out of {total_instances})\")\n",
    "        \n",
    "        # Collect and plot curves for this variable\n",
    "        curve_count = 0\n",
    "        for instance_id, group in df_act.groupby('activity_instance_id'):\n",
    "            group = group.sort_values('datetime_energy')\n",
    "            # Drop rows where either datetime_energy or variable is NaN\n",
    "            group = group.dropna(subset=['datetime_energy', variable])\n",
    "            values = group[variable].values\n",
    "            times = group['datetime_energy']\n",
    "            \n",
    "            if len(values) >= 1 and len(times) >= 1:  # Include curves with at least 1 point\n",
    "                # Calculate time elapsed in minutes from the start of this instance\n",
    "                time_elapsed = (times - times.min()).dt.total_seconds() / 60\n",
    "                \n",
    "                # Plot the curve\n",
    "                ax.plot(time_elapsed, values, alpha=0.7, linewidth=1, label=f'Instance {instance_id}')\n",
    "                curve_count += 1\n",
    "        \n",
    "        print(f\"  Number of curves plotted for {variable}: {curve_count}\")\n",
    "        \n",
    "        if curve_count == 0:\n",
    "            ax.text(0.5, 0.5, f'No valid curves for {variable}', \n",
    "                    ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        ax.set_title(f'{variable}')\n",
    "        ax.set_xlabel('Time (minutes)')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # If too many curves, don't show legend to avoid clutter\n",
    "        if curve_count > 10:\n",
    "            ax.legend().set_visible(False)\n",
    "        elif curve_count > 0:\n",
    "            ax.legend(fontsize=6, loc='best')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'Complete Energy Curves for Activity: {activity} (All Variables)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import optuna  # Assuming optuna is installed\n",
    "\n",
    "def train_position_based_regression(df_expanded, variable, activities, fixed_length=100, test_size=0.2, random_state=42, \n",
    "                                   models=None, optimize_hyperparams=False, n_trials=50):\n",
    "    \"\"\"\n",
    "    Train position-based regression models for energy curve prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_expanded : pd.DataFrame\n",
    "        The expanded dataframe with energy data\n",
    "    variable : str\n",
    "        The energy variable column name\n",
    "    activities : list\n",
    "        List of activity names to include\n",
    "    fixed_length : int\n",
    "        Fixed length to resample curves to\n",
    "    test_size : float\n",
    "        Proportion of data for testing\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "    models : dict, optional\n",
    "        Dictionary of model names to model classes (e.g., {'Gradient Boosting': GradientBoostingRegressor})\n",
    "        If None, defaults to GradientBoostingRegressor and RandomForestRegressor\n",
    "    optimize_hyperparams : bool, optional\n",
    "        Whether to perform hyperparameter optimization with Optuna (default: False)\n",
    "    n_trials : int, optional\n",
    "        Number of Optuna trials for optimization (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Results containing models, best model, and metadata\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"POSITION-BASED REGRESSION - NO ERROR COMPOUNDING!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Default models if not provided\n",
    "    if models is None:\n",
    "        models = {\n",
    "            'Gradient Boosting': GradientBoostingRegressor,\n",
    "            'Random Forest': RandomForestRegressor\n",
    "        }\n",
    "    \n",
    "    # Prepare data\n",
    "    df_plot = df_expanded.copy()\n",
    "    # Select necessary columns including object_attributes_log\n",
    "    df_plot = df_plot[['case_id_log', 'activity_log', 'timestamp_start_log', 'datetime_energy', variable, 'object_attributes_log']]\n",
    "    df_plot = df_plot[df_plot['activity_log'].isin(activities)]\n",
    "    df_plot['timestamp_start'] = pd.to_datetime(df_plot['timestamp_start_log'])\n",
    "    df_plot['datetime_energy'] = pd.to_datetime(df_plot['datetime_energy'])\n",
    "    df_plot = df_plot.sort_values(['case_id_log', 'timestamp_start_log', 'datetime_energy'])\n",
    "    \n",
    "    # Create instance IDs\n",
    "    df_plot['activity_instance_id'] = (\n",
    "        df_plot.groupby(['case_id_log', 'activity_log', 'timestamp_start_log']).ngroup()\n",
    "    )\n",
    "    \n",
    "    # Extract curves and attributes\n",
    "    curves_data = []\n",
    "    for instance_id, group in df_plot.groupby('activity_instance_id'):\n",
    "        group = group.sort_values('datetime_energy').reset_index(drop=True)\n",
    "        values = group[variable].dropna().values\n",
    "        \n",
    "        if len(values) >= 5:\n",
    "            # Get attributes from the first row (assuming same per instance)\n",
    "            attributes = group['object_attributes_log'].iloc[0] if not group['object_attributes_log'].empty else {}\n",
    "            \n",
    "            curves_data.append({\n",
    "                'instance_id': instance_id,\n",
    "                'activity': group['activity_log'].iloc[0],\n",
    "                'original_values': values,\n",
    "                'original_length': len(values),\n",
    "                'attributes': attributes\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nResampling all curves to fixed length: {fixed_length} points\")\n",
    "    \n",
    "    # Resample all curves to fixed length\n",
    "    for curve in curves_data:\n",
    "        x_old = np.linspace(0, 1, len(curve['original_values']))\n",
    "        x_new = np.linspace(0, 1, fixed_length)\n",
    "        curve['resampled_values'] = np.interp(x_new, x_old, curve['original_values'])\n",
    "    \n",
    "    print(f\"Total curves: {len(curves_data)}\")\n",
    "    \n",
    "    # Determine attribute columns and types\n",
    "    if curves_data:\n",
    "        all_keys = set()\n",
    "        for curve in curves_data:\n",
    "            all_keys.update(curve['attributes'].keys())\n",
    "        all_keys = sorted(all_keys)\n",
    "        \n",
    "        # Check convertibility for each key\n",
    "        key_types = {}\n",
    "        for key in all_keys:\n",
    "            is_numeric = True\n",
    "            for curve in curves_data:\n",
    "                if key in curve['attributes']:\n",
    "                    try:\n",
    "                        float(curve['attributes'][key])\n",
    "                    except (ValueError, TypeError):\n",
    "                        is_numeric = False\n",
    "                        break\n",
    "            key_types[key] = 'numeric' if is_numeric else 'category'\n",
    "        \n",
    "        print(f\"Attribute columns: {key_types}\")\n",
    "    else:\n",
    "        all_keys = []\n",
    "        key_types = {}\n",
    "    \n",
    "    # Build position-based regression dataset\n",
    "    regression_data = []\n",
    "    \n",
    "    for curve in curves_data:\n",
    "        for position_idx in range(fixed_length):\n",
    "            # Position as fraction (0 to 1)\n",
    "            position_fraction = position_idx / (fixed_length - 1) if fixed_length > 1 else 0\n",
    "            \n",
    "            row = {\n",
    "                'instance_id': curve['instance_id'],\n",
    "                'activity': curve['activity'],\n",
    "                'position_idx': position_idx,\n",
    "                'position_fraction': position_fraction,\n",
    "                'curve_length': curve['original_length'],\n",
    "                'y': curve['resampled_values'][position_idx]\n",
    "            }\n",
    "            \n",
    "            # Add attributes\n",
    "            for key in all_keys:\n",
    "                value = curve['attributes'].get(key, None)\n",
    "                if key_types[key] == 'numeric':\n",
    "                    try:\n",
    "                        row[key] = float(value) if value is not None else np.nan\n",
    "                    except (ValueError, TypeError):\n",
    "                        row[key] = np.nan\n",
    "                else:\n",
    "                    row[key] = str(value) if value is not None else 'None'\n",
    "            \n",
    "            regression_data.append(row)\n",
    "    \n",
    "    df_regression = pd.DataFrame(regression_data)\n",
    "    \n",
    "    print(f\"Regression dataset: {len(df_regression)} samples\")\n",
    "    print(f\"  {len(curves_data)} curves × {fixed_length} points\")\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = ['position_idx', 'position_fraction', 'curve_length']\n",
    "    X = df_regression[feature_cols].copy()\n",
    "    y = df_regression['y'].copy()\n",
    "    \n",
    "    # Add activity\n",
    "    X['activity'] = df_regression['activity']\n",
    "    \n",
    "    # Add attributes\n",
    "    categorical_cols = ['activity']\n",
    "    for key in all_keys:\n",
    "        if key_types[key] == 'numeric':\n",
    "            X[key] = df_regression[key]\n",
    "        else:\n",
    "            X[key] = df_regression[key]\n",
    "            categorical_cols.append(key)\n",
    "    \n",
    "    # One-hot encode categorical columns\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Split by COMPLETE CURVES\n",
    "    unique_instances = df_regression['instance_id'].unique()\n",
    "    train_instances, test_instances = train_test_split(\n",
    "        unique_instances, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    train_mask = df_regression['instance_id'].isin(train_instances)\n",
    "    test_mask = df_regression['instance_id'].isin(test_instances)\n",
    "    \n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "    \n",
    "    print(f\"\\nTrain/Test split:\")\n",
    "    print(f\"  Train: {len(train_instances)} curves, {len(X_train)} points\")\n",
    "    print(f\"  Test: {len(test_instances)} curves, {len(X_test)} points\")\n",
    "    \n",
    "    # Train models\n",
    "    results_position = {}\n",
    "    \n",
    "    for name, model_class in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        if optimize_hyperparams:\n",
    "            # Define objective function for Optuna\n",
    "            def objective(trial):\n",
    "                if name == 'Gradient Boosting':\n",
    "                    params = {\n",
    "                        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                        'random_state': random_state\n",
    "                    }\n",
    "                elif name == 'Random Forest':\n",
    "                    params = {\n",
    "                        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "                        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                        'random_state': random_state,\n",
    "                        'n_jobs': -1\n",
    "                    }\n",
    "                else:\n",
    "                    # For other models, no hyperparameter optimization defined\n",
    "                    params = {}\n",
    "                \n",
    "                model = model_class(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                return r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Run Optuna study\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=n_trials)\n",
    "            \n",
    "            # Get best parameters\n",
    "            best_params = study.best_params\n",
    "            if name == 'Gradient Boosting':\n",
    "                best_params['random_state'] = random_state\n",
    "            elif name == 'Random Forest':\n",
    "                best_params['random_state'] = random_state\n",
    "                best_params['n_jobs'] = -1\n",
    "            \n",
    "            print(f\"  Best params: {best_params}\")\n",
    "            model = model_class(**best_params)\n",
    "        else:\n",
    "            # Use default parameters\n",
    "            if name == 'Gradient Boosting':\n",
    "                model = model_class(\n",
    "                    n_estimators=200, max_depth=7, learning_rate=0.1,\n",
    "                    subsample=0.8, random_state=random_state\n",
    "                )\n",
    "            elif name == 'Random Forest':\n",
    "                model = model_class(\n",
    "                    n_estimators=200, max_depth=12, min_samples_split=5,\n",
    "                    random_state=random_state, n_jobs=-1\n",
    "                )\n",
    "            else:\n",
    "                # For other models, try with random_state, if not supported, without\n",
    "                try:\n",
    "                    model = model_class(random_state=random_state)\n",
    "                except TypeError:\n",
    "                    model = model_class()\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        \n",
    "        results_position[name] = {\n",
    "            'model': model,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse\n",
    "        }\n",
    "        \n",
    "        print(f\"  Train R²={train_r2:.4f}, RMSE={train_rmse:.2f}\")\n",
    "        print(f\"  Test R²={test_r2:.4f}, RMSE={test_rmse:.2f}\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name_pos = max(results_position.keys(), key=lambda k: results_position[k]['test_r2'])\n",
    "    best_model_pos = results_position[best_model_name_pos]['model']\n",
    "    \n",
    "    print(f\"\\n✓ Best model: {best_model_name_pos}\")\n",
    "    print(f\"  Test R² = {results_position[best_model_name_pos]['test_r2']:.4f}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    fixed_curve_length = fixed_length\n",
    "    feature_columns_pos = X.columns.tolist()\n",
    "    \n",
    "    return {\n",
    "        'results': results_position,\n",
    "        'best_model': best_model_pos,\n",
    "        'best_model_name': best_model_name_pos,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'feature_columns': feature_columns_pos,\n",
    "        'fixed_length': fixed_curve_length,\n",
    "        'curves_data': curves_data,\n",
    "        'test_instances': test_instances\n",
    "    }\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics: MAE, RMSE, WAPE, R²\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "    model_name : str\n",
    "        Name of the model for display\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Metrics dictionary\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # WAPE (Weighted Absolute Percentage Error)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    wape = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'WAPE (%)': wape,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "def display_metrics_table(results_dict, y_test, X_test):\n",
    "    \"\"\"\n",
    "    Display a table of metrics for all models in results_dict\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Results from training function\n",
    "    y_test : array-like\n",
    "        Test true values\n",
    "    X_test : pd.DataFrame\n",
    "        Test features\n",
    "    \"\"\"\n",
    "    metrics_list = []\n",
    "    \n",
    "    for model_name, model_info in results_dict['results'].items():\n",
    "        model = model_info['model']\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics = calculate_metrics(y_test, y_pred, model_name)\n",
    "        metrics_list.append(metrics)\n",
    "    \n",
    "    # Create DataFrame for table\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # Display table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(metrics_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_row = metrics_df.loc[metrics_df['R²'].idxmax()]\n",
    "    print(f\"\\n✓ Best model: {best_row['Model']} (R² = {best_row['R²']:.4f})\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Example usage:\n",
    "# training_results = train_position_based_regression(df_expanded, 'flow_Kuehlturmwasser_30120FT701_5s_energy', ['Step-030 = Produktion', 'Step-032 = Umlauf'])\n",
    "# metrics_table = display_metrics_table(training_results, training_results['y_test'], training_results['X_test'])\n",
    "\n",
    "# Example with custom models and optimization:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "custom_models = {'Linear Regression': LinearRegression, 'Gradient Boosting': GradientBoostingRegressor}\n",
    "training_results = train_position_based_regression(df_expanded, 'flow_Kuehlturmwasser_30120FT701_5s_energy', ['Step-030 = Produktion', 'Step-032 = Umlauf'], models=custom_models, optimize_hyperparams=False, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_energy_curves(activity, variable, df_expanded):\n",
    "    \"\"\"\n",
    "    Explore and visualize energy curves for a specific activity and variable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    activity : str\n",
    "        The activity name to filter for (e.g., 'Step-032 = Umlauf')\n",
    "    variable : str\n",
    "        The energy variable column name (e.g., 'flow_Kuehlturmwasser_30120FT701_5s_energy')\n",
    "    df_expanded : pd.DataFrame\n",
    "        The expanded dataframe with energy data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None : Displays the plot\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"Processing activity: {activity}\")\n",
    "    print(f\"Variable: {variable}\")\n",
    "    \n",
    "    # Filter for this activity\n",
    "    df_act = df_expanded[df_expanded['activity_log'] == activity].copy()\n",
    "    \n",
    "    # Create activity_instance_id\n",
    "    df_act['activity_instance_id'] = df_act.groupby(['case_id_log', 'activity_log', 'timestamp_start_log']).ngroup()\n",
    "    \n",
    "    # Calculate total number of instances for this activity\n",
    "    total_instances = df_act['activity_instance_id'].nunique()\n",
    "    print(f\"Total number of instances for {activity}: {total_instances}\")\n",
    "    \n",
    "    # Count instances with data for this variable\n",
    "    instances_with_data = 0\n",
    "    for instance_id in df_act['activity_instance_id'].unique():\n",
    "        instance_data = df_act[df_act['activity_instance_id'] == instance_id]\n",
    "        if instance_data[variable].notna().any():  # At least one non-NaN value\n",
    "            instances_with_data += 1\n",
    "    \n",
    "    print(f\"{variable}: {instances_with_data} instances have data (out of {total_instances})\")\n",
    "    \n",
    "    # Create a single plot for this variable\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Collect and plot curves for this variable\n",
    "    curve_count = 0\n",
    "    for instance_id, group in df_act.groupby('activity_instance_id'):\n",
    "        group = group.sort_values('datetime_energy')\n",
    "        # Drop rows where either datetime_energy or variable is NaN\n",
    "        group = group.dropna(subset=['datetime_energy', variable])\n",
    "        values = group[variable].values\n",
    "        times = group['datetime_energy']\n",
    "        \n",
    "        if len(values) >= 1 and len(times) >= 1:  # Include curves with at least 1 point\n",
    "            # Calculate time elapsed in minutes from the start of this instance\n",
    "            time_elapsed = (times - times.min()).dt.total_seconds() / 60\n",
    "            \n",
    "            # Plot the curve\n",
    "            ax.plot(time_elapsed, values, alpha=0.7, linewidth=1, label=f'Instance {instance_id}')\n",
    "            curve_count += 1\n",
    "    \n",
    "    print(f\"Number of curves plotted for {variable}: {curve_count}\")\n",
    "    \n",
    "    if curve_count == 0:\n",
    "        ax.text(0.5, 0.5, f'No valid curves for {variable}', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    ax.set_title(f'{variable} - Original Curves for {activity}')\n",
    "    ax.set_xlabel('Time (minutes)')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # If too many curves, don't show legend to avoid clutter\n",
    "    if curve_count > 10:\n",
    "        ax.legend().set_visible(False)\n",
    "    elif curve_count > 0:\n",
    "        ax.legend(fontsize=6, loc='best')\n",
    "    \n",
    "    plt.suptitle(f'Complete Energy Curves for Activity: {activity} ({variable})', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "explore_energy_curves('Step-032 = Umlauf', 'flow_Kuehlturmwasser_30120FT701_5s_energy', df_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energy_profile_extractor import EnergyProfileExtractor\n",
    "\n",
    "# Filter and prepare the data from df_expanded\n",
    "df_plot = df_expanded.copy()\n",
    "df_plot = df_plot[df_plot['activity_log'].isin(['Step-032 = Umlauf', 'Step-030 = Produktion'])]  # Adjust activity name if needed based on df_expanded['activity_log'].unique()\n",
    "df_plot['timestamp_start'] = pd.to_datetime(df_plot['timestamp_start_log'])  # Use the correct column name from df_expanded\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'temp_nach_Erhitzer_(WT5)_5s_energy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energy_profile_extractor import EnergyProfileExtractor\n",
    "\n",
    "activities = ['Step-032 = Umlauf', 'Step-030 = Produktion']\n",
    "activities = ['Step-030 = Produktion']\n",
    "\n",
    "# Filter and prepare the data from df_expanded\n",
    "df_plot = df_expanded.copy()\n",
    "df_plot = df_plot[df_plot['activity_log'].isin(activities)]\n",
    "df_plot['timestamp_start'] = pd.to_datetime(df_plot['timestamp_start_log'])\n",
    "df_plot['datetime_energy'] = pd.to_datetime(df_plot['datetime_energy'])\n",
    "\n",
    "df = df_plot.copy()\n",
    "\n",
    "# CRITICAL: Add start_end markers using datetime_energy (the actual time-series timestamp)\n",
    "# This ensures consistency with temporal order\n",
    "df['start_end'] = 'in_between'\n",
    "for (case_id, activity), group in df.groupby(['case_id_log', 'activity_log']):\n",
    "    group = group.sort_values('datetime_energy')  # Sort by actual energy timestamp\n",
    "    df.loc[group.index[0], 'start_end'] = 'start'\n",
    "    df.loc[group.index[-1], 'start_end'] = 'end'\n",
    "\n",
    "# Data quality check\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Unique case_id: {df['case_id_log'].nunique()}\")\n",
    "print(f\"Unique activities: {df['activity_log'].nunique()}\")\n",
    "print(f\"Start markers: {(df['start_end'] == 'start').sum()}\")\n",
    "print(f\"End markers: {(df['start_end'] == 'end').sum()}\")\n",
    "print(f\"NaN in value column: {df[variable].isna().sum()}\")\n",
    "\n",
    "# Initialize the extractor - USE datetime_energy as timestamp for consistency!\n",
    "extractor = EnergyProfileExtractor(\n",
    "    value_column=variable,\n",
    "    case_id_column='case_id_log',\n",
    "    activity_column='activity_log',\n",
    "    start_end_column='start_end',\n",
    "    timestamp_column='datetime_energy',  # Changed to match sorting column\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ENERGY PROFILE MODELS\")\n",
    "print(\"=\"*60)\n",
    "results_df = extractor.fit(df)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Check data structure before modeling\n",
    "print(\"=\"*60)\n",
    "print(\"DATA STRUCTURE DIAGNOSTIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check basic structure\n",
    "print(f\"\\nTotal rows in df_expanded: {len(df_expanded)}\")\n",
    "print(f\"Unique case_ids: {df_expanded['case_id_log'].nunique()}\")\n",
    "print(f\"Unique activities: {df_expanded['activity_log'].nunique()}\")\n",
    "\n",
    "# Filter to just the activity we're analyzing\n",
    "activities = ['Step-030 = Produktion']\n",
    "df_diagnostic = df_expanded[df_expanded['activity_log'].isin(activities)].copy()\n",
    "\n",
    "print(f\"\\nAfter filtering to {activities}:\")\n",
    "print(f\"Total rows: {len(df_diagnostic)}\")\n",
    "print(f\"Unique case_ids: {df_diagnostic['case_id_log'].nunique()}\")\n",
    "\n",
    "# Check how many energy time-series points per case_id + activity\n",
    "points_per_case = df_diagnostic.groupby(['case_id_log', 'activity_log']).size()\n",
    "print(f\"\\nTime-series points per (case_id, activity) instance:\")\n",
    "print(f\"  Min: {points_per_case.min()}\")\n",
    "print(f\"  Max: {points_per_case.max()}\")\n",
    "print(f\"  Mean: {points_per_case.mean():.1f}\")\n",
    "print(f\"  Median: {points_per_case.median():.1f}\")\n",
    "\n",
    "# Check a sample case\n",
    "sample_case = df_diagnostic['case_id_log'].iloc[0]\n",
    "sample_data = df_diagnostic[df_diagnostic['case_id_log'] == sample_case].copy()\n",
    "sample_data = sample_data.sort_values('datetime_energy')\n",
    "\n",
    "print(f\"\\nSample case_id '{sample_case}':\")\n",
    "print(f\"  Number of energy timestamps: {len(sample_data)}\")\n",
    "print(f\"  Time range: {sample_data['datetime_energy'].min()} to {sample_data['datetime_energy'].max()}\")\n",
    "print(f\"  Duration: {(sample_data['datetime_energy'].max() - sample_data['datetime_energy'].min()).total_seconds() / 60:.1f} minutes\")\n",
    "\n",
    "# Check for NaN values in energy variables\n",
    "nan_count = df_diagnostic[variable].isna().sum()\n",
    "nan_pct = (nan_count / len(df_diagnostic)) * 100\n",
    "print(f\"\\nNaN values in '{variable}':\")\n",
    "print(f\"  Count: {nan_count} ({nan_pct:.1f}%)\")\n",
    "\n",
    "# Check energy value statistics\n",
    "print(f\"\\nEnergy value statistics for '{variable}':\")\n",
    "print(df_diagnostic[variable].describe())\n",
    "\n",
    "# WARNING: Check if timestamps are properly aligned\n",
    "print(f\"\\nTimestamp alignment check:\")\n",
    "print(f\"  timestamp_start_log dtype: {df_diagnostic['timestamp_start_log'].dtype}\")\n",
    "print(f\"  datetime_energy dtype: {df_diagnostic['datetime_energy'].dtype}\")\n",
    "\n",
    "# Show first few rows with key columns\n",
    "print(f\"\\nFirst 10 rows of sample case:\")\n",
    "display(sample_data[['case_id_log', 'activity_log', 'timestamp_start_log', \n",
    "                      'timestamp_end_log', 'datetime_energy', variable]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Check start_end marker assignment\n",
    "print(\"=\"*60)\n",
    "print(\"START_END MARKER DIAGNOSTIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "activities = ['Step-030 = Produktion']\n",
    "\n",
    "# Prepare data\n",
    "df_plot = df_expanded.copy()\n",
    "df_plot = df_plot[df_plot['activity_log'].isin(activities)]\n",
    "df_plot['timestamp_start'] = pd.to_datetime(df_plot['timestamp_start_log'])\n",
    "df_plot['datetime_energy'] = pd.to_datetime(df_plot['datetime_energy'])\n",
    "\n",
    "df = df_plot.copy()\n",
    "\n",
    "# Add start_end markers\n",
    "df['start_end'] = 'in_between'\n",
    "for (case_id, activity), group in df.groupby(['case_id_log', 'activity_log']):\n",
    "    group = group.sort_values('datetime_energy')\n",
    "    df.loc[group.index[0], 'start_end'] = 'start'\n",
    "    df.loc[group.index[-1], 'start_end'] = 'end'\n",
    "\n",
    "# Check marker distribution\n",
    "print(f\"\\nStart_end marker distribution:\")\n",
    "print(df['start_end'].value_counts())\n",
    "\n",
    "# Check a few sample cases\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Sample case_id analysis:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "unique_cases = df['case_id_log'].unique()[:3]  # Check first 3 cases\n",
    "\n",
    "for case_id in unique_cases:\n",
    "    case_data = df[df['case_id_log'] == case_id].sort_values('datetime_energy')\n",
    "    \n",
    "    print(f\"\\nCase ID: {case_id}\")\n",
    "    print(f\"  Total energy timestamps: {len(case_data)}\")\n",
    "    print(f\"  Start markers: {(case_data['start_end'] == 'start').sum()}\")\n",
    "    print(f\"  End markers: {(case_data['start_end'] == 'end').sum()}\")\n",
    "    print(f\"  In-between markers: {(case_data['start_end'] == 'in_between').sum()}\")\n",
    "    \n",
    "    # Show first and last few rows\n",
    "    print(f\"\\n  First 3 timestamps:\")\n",
    "    display(case_data[['datetime_energy', 'start_end', variable]].head(3))\n",
    "    \n",
    "    print(f\"\\n  Last 3 timestamps:\")\n",
    "    display(case_data[['datetime_energy', 'start_end', variable]].tail(3))\n",
    "    \n",
    "    # Check for issues\n",
    "    if (case_data['start_end'] == 'start').sum() != 1:\n",
    "        print(f\"  ⚠️ WARNING: Should have exactly 1 'start' marker!\")\n",
    "    if (case_data['start_end'] == 'end').sum() != 1:\n",
    "        print(f\"  ⚠️ WARNING: Should have exactly 1 'end' marker!\")\n",
    "    if len(case_data) < 3:\n",
    "        print(f\"  ⚠️ WARNING: Very short curve (< 3 points)!\")\n",
    "\n",
    "# Check for data quality issues\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA QUALITY CHECKS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Unique case_id: {df['case_id_log'].nunique()}\")\n",
    "print(f\"Unique activities: {df['activity_log'].nunique()}\")\n",
    "print(f\"Start markers: {(df['start_end'] == 'start').sum()}\")\n",
    "print(f\"End markers: {(df['start_end'] == 'end').sum()}\")\n",
    "print(f\"NaN in value column: {df[variable].isna().sum()} ({df[variable].isna().sum() / len(df) * 100:.1f}%)\")\n",
    "print(f\"\\nExpected: start markers == end markers == unique case_id count\")\n",
    "print(f\"Actual: {(df['start_end'] == 'start').sum()} == {(df['start_end'] == 'end').sum()} == {df['case_id_log'].nunique()}\")\n",
    "\n",
    "if (df['start_end'] == 'start').sum() != df['case_id_log'].nunique():\n",
    "    print(\"\\n⚠️ MISMATCH DETECTED! This will cause problems with curve extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Potential Issues and Fixes\n",
    "\n",
    "Based on the diagnostics above, here are common issues and their solutions:\n",
    "\n",
    "### Issue 1: Overlapping Activities\n",
    "If you have multiple occurrences of the same activity for the same case_id (e.g., the activity happens multiple times), you need to create a unique identifier for each activity instance.\n",
    "\n",
    "### Issue 2: Too Few or Too Many Energy Points\n",
    "- If curves have < 3 points, they'll be skipped\n",
    "- If there's too much variation in curve lengths, the model may struggle\n",
    "\n",
    "### Issue 3: NaN Values\n",
    "- If > 50% of values in a curve are NaN, the curve is skipped\n",
    "- Make sure energy data is properly aligned with activity timestamps\n",
    "\n",
    "### Issue 4: Timestamp Misalignment\n",
    "- `timestamp_start_log` and `timestamp_end_log` define the activity boundaries\n",
    "- `datetime_energy` should fall within these boundaries\n",
    "- If energy timestamps don't align, curves will be empty or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED VERSION: Enhanced energy modeling with better data handling\n",
    "from energy_profile_extractor import EnergyProfileExtractor\n",
    "\n",
    "\n",
    "activities = ['Step-030 = Produktion']\n",
    "\n",
    "# Filter and prepare the data\n",
    "df_plot = df_expanded.copy()\n",
    "df_plot = df_plot[df_plot['activity_log'].isin(activities)]\n",
    "df_plot['timestamp_start'] = pd.to_datetime(df_plot['timestamp_start_log'])\n",
    "df_plot['datetime_energy'] = pd.to_datetime(df_plot['datetime_energy'])\n",
    "\n",
    "# CRITICAL FIX: Create unique activity instance IDs\n",
    "# This handles cases where the same activity occurs multiple times for one case_id\n",
    "print(\"Creating unique activity instance IDs...\")\n",
    "df_plot = df_plot.sort_values(['case_id_log', 'timestamp_start_log', 'datetime_energy'])\n",
    "\n",
    "# Create a unique instance ID for each distinct (case_id, activity, timestamp_start) combination\n",
    "df_plot['activity_instance_id'] = (\n",
    "    df_plot.groupby(['case_id_log', 'activity_log', 'timestamp_start_log'])\n",
    "    .ngroup()\n",
    "    .astype(str)\n",
    ")\n",
    "\n",
    "# Also create a more readable instance name\n",
    "df_plot['instance_name'] = (\n",
    "    df_plot['case_id_log'].astype(str) + '_' + \n",
    "    df_plot['activity_log'].astype(str) + '_' +\n",
    "    df_plot.groupby(['case_id_log', 'activity_log']).cumcount().astype(str)\n",
    ")\n",
    "\n",
    "df = df_plot.copy()\n",
    "\n",
    "# Add start_end markers using the unique instance ID\n",
    "df['start_end'] = 'in_between'\n",
    "for instance_id, group in df.groupby('activity_instance_id'):\n",
    "    group = group.sort_values('datetime_energy')\n",
    "    if len(group) >= 3:  # Only process groups with at least 3 points\n",
    "        df.loc[group.index[0], 'start_end'] = 'start'\n",
    "        df.loc[group.index[-1], 'start_end'] = 'end'\n",
    "    else:\n",
    "        # Mark short curves for removal\n",
    "        df.loc[group.index, 'start_end'] = 'too_short'\n",
    "\n",
    "# Remove curves that are too short\n",
    "df = df[df['start_end'] != 'too_short']\n",
    "\n",
    "# Data quality report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENHANCED DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows (energy timestamps): {len(df)}\")\n",
    "print(f\"Unique case_ids: {df['case_id_log'].nunique()}\")\n",
    "print(f\"Unique activities: {df['activity_log'].nunique()}\")\n",
    "print(f\"Unique activity instances: {df['activity_instance_id'].nunique()}\")\n",
    "print(f\"Start markers: {(df['start_end'] == 'start').sum()}\")\n",
    "print(f\"End markers: {(df['start_end'] == 'end').sum()}\")\n",
    "print(f\"In-between markers: {(df['start_end'] == 'in_between').sum()}\")\n",
    "print(f\"NaN in value column: {df[variable].isna().sum()} ({df[variable].isna().sum() / len(df) * 100:.1f}%)\")\n",
    "\n",
    "# Check points per curve\n",
    "points_per_curve = df.groupby('activity_instance_id').size()\n",
    "print(f\"\\nPoints per activity instance curve:\")\n",
    "print(f\"  Min: {points_per_curve.min()}\")\n",
    "print(f\"  Max: {points_per_curve.max()}\")\n",
    "print(f\"  Mean: {points_per_curve.mean():.1f}\")\n",
    "print(f\"  Median: {points_per_curve.median():.1f}\")\n",
    "\n",
    "# Validate: starts == ends == unique instances\n",
    "is_valid = (\n",
    "    (df['start_end'] == 'start').sum() == \n",
    "    (df['start_end'] == 'end').sum() == \n",
    "    df['activity_instance_id'].nunique()\n",
    ")\n",
    "print(f\"\\n✓ Data validation: {'PASSED' if is_valid else 'FAILED'}\")\n",
    "\n",
    "if not is_valid:\n",
    "    print(\"⚠️ WARNING: Data structure issues detected!\")\n",
    "    print(f\"  Expected starts = ends = instances\")\n",
    "    print(f\"  Got: {(df['start_end'] == 'start').sum()} starts, {(df['start_end'] == 'end').sum()} ends, {df['activity_instance_id'].nunique()} instances\")\n",
    "else:\n",
    "    # Initialize the extractor with the unique instance ID as case_id\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING ENERGY PROFILE MODELS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    extractor = EnergyProfileExtractor(\n",
    "        value_column=variable,\n",
    "        case_id_column='activity_instance_id',  # Use unique instance ID\n",
    "        activity_column='activity_log',\n",
    "        start_end_column='start_end',\n",
    "        timestamp_column='datetime_energy',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit on training data\n",
    "    try:\n",
    "        results_df = extractor.fit(df)\n",
    "        display(results_df)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ ERROR during fitting: {e}\")\n",
    "        print(\"\\nShowing first few rows of problematic data:\")\n",
    "        display(df[['activity_instance_id', 'activity_log', 'datetime_energy', \n",
    "                    'start_end', variable]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEP DIAGNOSTIC: Analyze extracted curves and model inputs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DEEP DIAGNOSTIC: INVESTIGATING POOR MODEL PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Analyze the extracted curves\n",
    "print(\"\\n1. ANALYZING EXTRACTED CURVES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if hasattr(extractor, 'curves_') and extractor.curves_:\n",
    "    curves = extractor.curves_\n",
    "    \n",
    "    print(f\"Total curves extracted: {len(curves)}\")\n",
    "    print(f\"Target resampled length: {extractor.target_length_}\")\n",
    "    \n",
    "    # Analyze curve statistics\n",
    "    curve_lengths_original = []\n",
    "    curve_means = []\n",
    "    curve_stds = []\n",
    "    curve_mins = []\n",
    "    curve_maxs = []\n",
    "    \n",
    "    for curve in curves:\n",
    "        values = curve['values']\n",
    "        curve_means.append(np.mean(values))\n",
    "        curve_stds.append(np.std(values))\n",
    "        curve_mins.append(np.min(values))\n",
    "        curve_maxs.append(np.max(values))\n",
    "    \n",
    "    print(f\"\\nCurve value statistics (after resampling to {extractor.target_length_} points):\")\n",
    "    print(f\"  Mean values across curves: min={min(curve_means):.2f}, max={max(curve_means):.2f}, avg={np.mean(curve_means):.2f}\")\n",
    "    print(f\"  Std values across curves: min={min(curve_stds):.2f}, max={max(curve_stds):.2f}, avg={np.mean(curve_stds):.2f}\")\n",
    "    print(f\"  Overall value range: {min(curve_mins):.2f} to {max(curve_maxs):.2f}\")\n",
    "    \n",
    "    # Check for problematic patterns\n",
    "    zero_std_curves = sum(1 for std in curve_stds if std < 0.01)\n",
    "    identical_curves = sum(1 for mean in curve_means if abs(mean - curve_means[0]) < 0.01)\n",
    "    \n",
    "    print(f\"\\nPotential issues:\")\n",
    "    print(f\"  Flat curves (std < 0.01): {zero_std_curves}\")\n",
    "    print(f\"  Nearly identical mean values: {identical_curves}\")\n",
    "    \n",
    "    # Visualize sample curves\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('Diagnostic: Extracted Energy Curves', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: First 10 curves\n",
    "    ax1 = axes[0, 0]\n",
    "    for i, curve in enumerate(curves[:10]):\n",
    "        ax1.plot(curve['values'], alpha=0.7, label=f'Curve {i+1}')\n",
    "    ax1.set_title(f'First 10 Curves (Resampled to {extractor.target_length_} points)')\n",
    "    ax1.set_xlabel('Time Index')\n",
    "    ax1.set_ylabel('Energy Value')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Distribution of mean values\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(curve_means, bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax2.set_title('Distribution of Curve Mean Values')\n",
    "    ax2.set_xlabel('Mean Energy Value')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Distribution of std values\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.hist(curve_stds, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "    ax3.set_title('Distribution of Curve Std Values')\n",
    "    ax3.set_xlabel('Std Deviation')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Value range analysis\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.scatter(curve_means, curve_stds, alpha=0.6)\n",
    "    ax4.set_title('Mean vs Std of Each Curve')\n",
    "    ax4.set_xlabel('Mean Value')\n",
    "    ax4.set_ylabel('Std Deviation')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Check the regression dataset\n",
    "    print(\"\\n2. ANALYZING REGRESSION DATASET\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Create the dataframe that the model sees\n",
    "    from energy_profile_extractor import EnergyProfileExtractor\n",
    "    df_model = extractor._curves_to_dataframe(curves)\n",
    "    \n",
    "    print(f\"Regression dataset shape: {df_model.shape}\")\n",
    "    print(f\"Columns: {df_model.columns.tolist()}\")\n",
    "    print(f\"\\nTarget variable (y) statistics:\")\n",
    "    print(df_model['y'].describe())\n",
    "    \n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(f\"  curve_index: min={df_model['curve_index'].min()}, max={df_model['curve_index'].max()}\")\n",
    "    print(f\"  Unique activities: {df_model[extractor.activity_column].nunique()}\")\n",
    "    \n",
    "    # Check for issues\n",
    "    print(f\"\\nData quality checks:\")\n",
    "    print(f\"  NaN values in y: {df_model['y'].isna().sum()}\")\n",
    "    print(f\"  Infinite values in y: {np.isinf(df_model['y']).sum()}\")\n",
    "    print(f\"  Zero values in y: {(df_model['y'] == 0).sum()}\")\n",
    "    print(f\"  Constant y values: {df_model['y'].nunique() == 1}\")\n",
    "    \n",
    "    # Check variance\n",
    "    print(f\"\\nVariance analysis:\")\n",
    "    print(f\"  Y variance: {df_model['y'].var():.4f}\")\n",
    "    print(f\"  Y range: [{df_model['y'].min():.2f}, {df_model['y'].max():.2f}]\")\n",
    "    \n",
    "    # 3. Visualize the regression problem\n",
    "    print(\"\\n3. VISUALIZING THE REGRESSION PROBLEM\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    fig.suptitle('Diagnostic: Regression Problem Visualization', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot: curve_index vs y for a sample of curves\n",
    "    ax1 = axes[0]\n",
    "    sample_curves = df_model['curve_id'].unique()[:5]\n",
    "    for curve_id in sample_curves:\n",
    "        curve_data = df_model[df_model['curve_id'] == curve_id]\n",
    "        ax1.plot(curve_data['curve_index'], curve_data['y'], alpha=0.7, marker='o', markersize=3, label=f'Curve {curve_id}')\n",
    "    ax1.set_title('Sample Curves: curve_index vs y')\n",
    "    ax1.set_xlabel('Curve Index (time step)')\n",
    "    ax1.set_ylabel('y (energy value)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot: Distribution of y values\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(df_model['y'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax2.set_title('Distribution of Target Variable (y)')\n",
    "    ax2.set_xlabel('y (energy value)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No curves extracted yet. Run the extractor.fit() method first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED APPROACH 1: NO DATA LEAKAGE - Realistic Curve Generation\n",
    "# Only use features available BEFORE/DURING curve generation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPROACH 1 FIXED: NO DATA LEAKAGE - AUTOREGRESSIVE GENERATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Only using features available at simulation time!\")\n",
    "\n",
    "\n",
    "activities = ['Step-030 = Produktion']\n",
    "\n",
    "df_plot = df_expanded.copy()\n",
    "df_plot = df_plot[df_plot['activity_log'].isin(activities)]\n",
    "df_plot['timestamp_start'] = pd.to_datetime(df_plot['timestamp_start_log'])\n",
    "df_plot['datetime_energy'] = pd.to_datetime(df_plot['datetime_energy'])\n",
    "df_plot = df_plot.sort_values(['case_id_log', 'timestamp_start_log', 'datetime_energy'])\n",
    "\n",
    "# Create unique instance IDs\n",
    "df_plot['activity_instance_id'] = (\n",
    "    df_plot.groupby(['case_id_log', 'activity_log', 'timestamp_start_log'])\n",
    "    .ngroup()\n",
    ")\n",
    "\n",
    "# Build features WITHOUT data leakage\n",
    "enhanced_data = []\n",
    "\n",
    "for instance_id, group in df_plot.groupby('activity_instance_id'):\n",
    "    group = group.sort_values('datetime_energy').reset_index(drop=True)\n",
    "    values = group[variable].dropna().values\n",
    "    \n",
    "    if len(values) >= 5:  # Minimum for meaningful features\n",
    "        for i in range(len(values)):\n",
    "            # Normalized position in curve (0 to 1)\n",
    "            normalized_pos = i / (len(values) - 1) if len(values) > 1 else 0\n",
    "            \n",
    "            # ONLY USE FEATURES AVAILABLE AT PREDICTION TIME\n",
    "            features = {\n",
    "                'instance_id': instance_id,\n",
    "                'activity': group['activity_log'].iloc[0],\n",
    "                \n",
    "                # ✓ Position features (known during generation)\n",
    "                'norm_index': normalized_pos,  # Where we are (0-1)\n",
    "                'remaining_ratio': 1 - normalized_pos,  # How much is left\n",
    "                \n",
    "                # ✓ Curve length (estimated duration is known)\n",
    "                'total_length': len(values),\n",
    "                'log_total_length': np.log1p(len(values)),\n",
    "                \n",
    "                # ✓ Autoregressive features (values generated so far)\n",
    "                'value_lag1': values[i-1] if i > 0 else 0,\n",
    "                'value_lag2': values[i-2] if i > 1 else 0,\n",
    "                'value_lag3': values[i-3] if i > 2 else 0,\n",
    "                'value_lag5': values[i-5] if i > 4 else 0,\n",
    "                \n",
    "                # ✓ Cumulative statistics (computed from values BEFORE current point)\n",
    "                'cumsum': np.sum(values[:i]) if i > 0 else 0,\n",
    "                'cummean': np.mean(values[:i]) if i > 0 else 0,\n",
    "                'cumstd': np.std(values[:i]) if i > 1 else 0,\n",
    "                'cummin': np.min(values[:i]) if i > 0 else 0,\n",
    "                'cummax': np.max(values[:i]) if i > 0 else 0,\n",
    "                \n",
    "                # ✓ Local trends (from history - NOT including current)\n",
    "                'diff_lag1': values[i-1] - values[i-2] if i > 1 else 0,\n",
    "                'diff_lag2': values[i-2] - values[i-3] if i > 2 else 0,\n",
    "                'acceleration': (values[i-1] - values[i-2]) - (values[i-2] - values[i-3]) if i > 2 else 0,\n",
    "                \n",
    "                # ✓ Rolling statistics (from history - NOT including current)\n",
    "                'rolling_mean_3': np.mean(values[max(0,i-3):i]) if i > 0 else 0,\n",
    "                'rolling_std_3': np.std(values[max(0,i-3):i]) if i > 2 else 0,\n",
    "                'rolling_mean_5': np.mean(values[max(0,i-5):i]) if i > 0 else 0,\n",
    "                'rolling_max_5': np.max(values[max(0,i-5):i]) if i > 0 else 0,\n",
    "                'rolling_min_5': np.min(values[max(0,i-5):i]) if i > 0 else 0,\n",
    "                \n",
    "                # ✓ Interaction features\n",
    "                'pos_x_lag1': normalized_pos * (values[i-1] if i > 0 else 0),\n",
    "                'pos_x_cummean': normalized_pos * (np.mean(values[:i]) if i > 0 else 0),\n",
    "                \n",
    "                # ❌ REMOVED: curve_mean, curve_std, curve_min, curve_max, curve_range\n",
    "                # These use future information not available at generation time!\n",
    "                \n",
    "                # Target\n",
    "                'y': values[i]\n",
    "            }\n",
    "            \n",
    "            enhanced_data.append(features)\n",
    "\n",
    "df_enhanced = pd.DataFrame(enhanced_data)\n",
    "\n",
    "print(f\"\\nCreated LEAK-FREE enhanced dataset:\")\n",
    "print(f\"  Total samples: {len(df_enhanced)}\")\n",
    "print(f\"  Unique instances: {df_enhanced['instance_id'].nunique()}\")\n",
    "print(f\"  Feature columns: {len([c for c in df_enhanced.columns if c not in ['instance_id', 'activity', 'y']])}\")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [c for c in df_enhanced.columns if c not in ['instance_id', 'activity', 'y']]\n",
    "X = df_enhanced[feature_cols].copy()\n",
    "y = df_enhanced['y'].copy()\n",
    "\n",
    "# Add activity as categorical\n",
    "X['activity'] = df_enhanced['activity']\n",
    "X = pd.get_dummies(X, columns=['activity'], drop_first=True)\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")\n",
    "\n",
    "# CRITICAL: Split by instance to prevent leakage\n",
    "unique_instances = df_enhanced['instance_id'].unique()\n",
    "train_instances, test_instances = train_test_split(unique_instances, test_size=0.2, random_state=42)\n",
    "\n",
    "train_mask = df_enhanced['instance_id'].isin(train_instances)\n",
    "test_mask = df_enhanced['instance_id'].isin(test_instances)\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "print(f\"\\nTrain/Test split (by instance):\")\n",
    "print(f\"  Train: {len(train_instances)} instances, {len(X_train)} samples\")\n",
    "print(f\"  Test: {len(test_instances)} instances, {len(X_test)} samples\")\n",
    "\n",
    "# Train models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MODELS (NO LEAKAGE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {\n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train RMSE: {train_rmse:.2f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.2f}, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    r2_diff = train_r2 - test_r2\n",
    "    if r2_diff > 0.2:\n",
    "        print(f\"  ⚠️ Warning: Overfitting detected (R² gap: {r2_diff:.3f})\")\n",
    "    \n",
    "    if test_r2 > 0.7:\n",
    "        print(f\"  ✓ Excellent performance!\")\n",
    "    elif test_r2 > 0.5:\n",
    "        print(f\"  ✓ Good performance!\")\n",
    "    elif test_r2 > 0.3:\n",
    "        print(f\"  ⚠️ Moderate performance\")\n",
    "    else:\n",
    "        print(f\"  ❌ Poor performance - may need more features or different approach\")\n",
    "\n",
    "# Show feature importance for best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['test_r2'])\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"FEATURE IMPORTANCE ({best_model_name})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 most important features:\")\n",
    "    display(importance_df.head(15))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Feature importance\n",
    "    axes[0].barh(importance_df['feature'][:15][::-1], importance_df['importance'][:15][::-1])\n",
    "    axes[0].set_xlabel('Importance')\n",
    "    axes[0].set_title('Top 15 Feature Importances')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predictions vs actual\n",
    "    axes[1].scatter(y_test, results[best_model_name]['predictions'], alpha=0.5, s=1)\n",
    "    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[1].set_xlabel('Actual Energy')\n",
    "    axes[1].set_ylabel('Predicted Energy')\n",
    "    axes[1].set_title(f'Predictions vs Actual (Test Set)\\nR² = {results[best_model_name][\"test_r2\"]:.4f}')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize AUTOREGRESSIVE curve generation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AUTOREGRESSIVE CURVE GENERATION (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Simulating realistic curve generation point-by-point\")\n",
    "\n",
    "test_instance_ids = df_enhanced[test_mask]['instance_id'].unique()[:4]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, instance_id in enumerate(test_instance_ids):\n",
    "    instance_mask = df_enhanced['instance_id'] == instance_id\n",
    "    instance_data = df_enhanced[instance_mask].reset_index(drop=True)\n",
    "    \n",
    "    y_actual = instance_data['y'].values\n",
    "    \n",
    "    # Autoregressive prediction (generate point-by-point)\n",
    "    y_generated = []\n",
    "    \n",
    "    for i in range(len(instance_data)):\n",
    "        # Build features for current point using only past generated values\n",
    "        row_features = instance_data.iloc[i][feature_cols].copy()\n",
    "        \n",
    "        # Update autoregressive features with generated values (not actual)\n",
    "        if i > 0:\n",
    "            row_features['value_lag1'] = y_generated[-1]\n",
    "            row_features['cumsum'] = np.sum(y_generated)\n",
    "            row_features['cummean'] = np.mean(y_generated)\n",
    "            row_features['cumstd'] = np.std(y_generated) if i > 1 else 0\n",
    "            row_features['cummin'] = np.min(y_generated)\n",
    "            row_features['cummax'] = np.max(y_generated)\n",
    "            row_features['pos_x_lag1'] = row_features['norm_index'] * y_generated[-1]\n",
    "            row_features['pos_x_cummean'] = row_features['norm_index'] * np.mean(y_generated)\n",
    "            row_features['rolling_mean_3'] = np.mean(y_generated[-3:]) if len(y_generated) >= 1 else 0\n",
    "            row_features['rolling_std_3'] = np.std(y_generated[-3:]) if len(y_generated) >= 3 else 0\n",
    "            row_features['rolling_mean_5'] = np.mean(y_generated[-5:]) if len(y_generated) >= 1 else 0\n",
    "            row_features['rolling_max_5'] = np.max(y_generated[-5:]) if len(y_generated) >= 1 else 0\n",
    "            row_features['rolling_min_5'] = np.min(y_generated[-5:]) if len(y_generated) >= 1 else 0\n",
    "        \n",
    "        if i > 1:\n",
    "            row_features['value_lag2'] = y_generated[-2]\n",
    "            row_features['diff_lag1'] = y_generated[-1] - y_generated[-2]\n",
    "        \n",
    "        if i > 2:\n",
    "            row_features['diff_lag2'] = y_generated[-2] - y_generated[-3]\n",
    "            row_features['acceleration'] = (y_generated[-1] - y_generated[-2]) - (y_generated[-2] - y_generated[-3])\n",
    "        \n",
    "        if i > 2:\n",
    "            row_features['value_lag3'] = y_generated[-3]\n",
    "        \n",
    "        if i > 4:\n",
    "            row_features['value_lag5'] = y_generated[-5]\n",
    "        \n",
    "        # Add activity encoding\n",
    "        activity_val = instance_data.iloc[i]['activity']\n",
    "        for col in X.columns:\n",
    "            if col.startswith('activity_'):\n",
    "                row_features[col] = 1 if col == f'activity_{activity_val}' else 0\n",
    "        \n",
    "        # Predict next point\n",
    "        X_pred = pd.DataFrame([row_features])[X.columns]\n",
    "        y_pred = best_model.predict(X_pred)[0]\n",
    "        y_generated.append(y_pred)\n",
    "    \n",
    "    y_generated = np.array(y_generated)\n",
    "    \n",
    "    # Calculate R² for this curve\n",
    "    curve_r2 = r2_score(y_actual, y_generated)\n",
    "    \n",
    "    axes[idx].plot(y_actual, label='Actual', linewidth=2, marker='o', markersize=4, alpha=0.7)\n",
    "    axes[idx].plot(y_generated, label='Generated (Autoregressive)', linewidth=2, \n",
    "                   marker='s', markersize=4, alpha=0.7, linestyle='--')\n",
    "    axes[idx].set_title(f'Instance {instance_id} (R²={curve_r2:.3f})')\n",
    "    axes[idx].set_xlabel('Time Step')\n",
    "    axes[idx].set_ylabel('Energy Value')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Autoregressive Energy Curve Generation (Test Set)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIXED APPROACH 1 SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"  Test R²: {results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"  Test RMSE: {results[best_model_name]['test_rmse']:.2f}\")\n",
    "\n",
    "print(\"\\n✓ This R² is REALISTIC because:\")\n",
    "print(\"  - No global curve statistics used\")\n",
    "print(\"  - Only features available at simulation time\")\n",
    "print(\"  - Train/test split by instance\")\n",
    "print(\"  - Autoregressive generation tested\")\n",
    "\n",
    "if results[best_model_name]['test_r2'] > 0.5:\n",
    "    print(\"\\n✓ Model is ready for simulation!\")\n",
    "    print(\"  You can generate realistic energy curves point-by-point\")\n",
    "elif results[best_model_name]['test_r2'] > 0.3:\n",
    "    print(\"\\n⚠️ Moderate performance - usable but could be improved\")\n",
    "    print(\"  Consider adding more process features (product type, batch size, etc.)\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Poor performance - need more information\")\n",
    "    print(\"  Try: Approach 2 (Fourier), add process parameters, or use historical curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Compare Test Set Performance vs Autoregressive Performance\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CRITICAL DIAGNOSTIC: FINDING THE REAL R²\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# VERIFICATION: Confirm we're using complete curves for train/test\n",
    "train_instance_ids = df_enhanced[train_mask]['instance_id'].unique()\n",
    "test_instance_ids = df_enhanced[test_mask]['instance_id'].unique()\n",
    "\n",
    "print(\"\\n✅ TRAIN/TEST SPLIT VERIFICATION:\")\n",
    "print(f\"   Training curves (instances): {len(train_instance_ids)}\")\n",
    "print(f\"   Test curves (instances): {len(test_instance_ids)}\")\n",
    "print(f\"   Total training points: {train_mask.sum()}\")\n",
    "print(f\"   Total test points: {test_mask.sum()}\")\n",
    "print(f\"   ✓ Using COMPLETE curves - no curve is split between train/test!\")\n",
    "\n",
    "# Verify no overlap\n",
    "overlap = set(train_instance_ids) & set(test_instance_ids)\n",
    "assert len(overlap) == 0, \"ERROR: Train and test instances overlap!\"\n",
    "print(f\"   ✓ Zero overlap between train and test curves\")\n",
    "\n",
    "print(f\"\\n📊 Now evaluating on ALL {len(test_instance_ids)} test curves...\")\n",
    "\n",
    "all_actual = []\n",
    "all_predicted_testset = []\n",
    "all_predicted_autoregressive = []\n",
    "\n",
    "for instance_id in test_instance_ids:\n",
    "    instance_mask = df_enhanced['instance_id'] == instance_id\n",
    "    instance_data = df_enhanced[instance_mask].reset_index(drop=True)\n",
    "    \n",
    "    y_actual = instance_data['y'].values\n",
    "    all_actual.extend(y_actual)\n",
    "    \n",
    "    # 1. Test set predictions (using ACTUAL lagged values)\n",
    "    y_pred_testset = best_model.predict(X[instance_mask])\n",
    "    all_predicted_testset.extend(y_pred_testset)\n",
    "    \n",
    "    # 2. Autoregressive predictions (using PREDICTED lagged values)\n",
    "    y_generated = []\n",
    "    \n",
    "    for i in range(len(instance_data)):\n",
    "        row_features = instance_data.iloc[i][feature_cols].copy()\n",
    "        \n",
    "        # Update with PREDICTED values (not actual)\n",
    "        if i > 0:\n",
    "            row_features['value_lag1'] = y_generated[-1]\n",
    "            row_features['cumsum'] = np.sum(y_generated)\n",
    "            row_features['cummean'] = np.mean(y_generated)\n",
    "            row_features['cumstd'] = np.std(y_generated) if i > 1 else 0\n",
    "            row_features['cummin'] = np.min(y_generated)\n",
    "            row_features['cummax'] = np.max(y_generated)\n",
    "            row_features['pos_x_lag1'] = row_features['norm_index'] * y_generated[-1]\n",
    "            row_features['pos_x_cummean'] = row_features['norm_index'] * np.mean(y_generated)\n",
    "            row_features['rolling_mean_3'] = np.mean(y_generated[-3:]) if len(y_generated) >= 1 else 0\n",
    "            row_features['rolling_std_3'] = np.std(y_generated[-3:]) if len(y_generated) >= 3 else 0\n",
    "            row_features['rolling_mean_5'] = np.mean(y_generated[-5:]) if len(y_generated) >= 1 else 0\n",
    "            row_features['rolling_max_5'] = np.max(y_generated[-5:]) if len(y_generated) >= 1 else 0\n",
    "            row_features['rolling_min_5'] = np.min(y_generated[-5:]) if len(y_generated) >= 1 else 0\n",
    "        \n",
    "        if i > 1:\n",
    "            row_features['value_lag2'] = y_generated[-2]\n",
    "            row_features['diff_lag1'] = y_generated[-1] - y_generated[-2]\n",
    "        \n",
    "        if i > 2:\n",
    "            row_features['diff_lag2'] = y_generated[-2] - y_generated[-3]\n",
    "            row_features['acceleration'] = (y_generated[-1] - y_generated[-2]) - (y_generated[-2] - y_generated[-3])\n",
    "        \n",
    "        if i > 2:\n",
    "            row_features['value_lag3'] = y_generated[-3]\n",
    "        \n",
    "        if i > 4:\n",
    "            row_features['value_lag5'] = y_generated[-5]\n",
    "        \n",
    "        # Activity encoding\n",
    "        activity_val = instance_data.iloc[i]['activity']\n",
    "        for col in X.columns:\n",
    "            if col.startswith('activity_'):\n",
    "                row_features[col] = 1 if col == f'activity_{activity_val}' else 0\n",
    "        \n",
    "        X_pred = pd.DataFrame([row_features])[X.columns]\n",
    "        y_pred = best_model.predict(X_pred)[0]\n",
    "        y_generated.append(y_pred)\n",
    "    \n",
    "    all_predicted_autoregressive.extend(y_generated)\n",
    "\n",
    "# Calculate both R² scores\n",
    "all_actual = np.array(all_actual)\n",
    "all_predicted_testset = np.array(all_predicted_testset)\n",
    "all_predicted_autoregressive = np.array(all_predicted_autoregressive)\n",
    "\n",
    "r2_testset = r2_score(all_actual, all_predicted_testset)\n",
    "r2_autoregressive = r2_score(all_actual, all_predicted_autoregressive)\n",
    "\n",
    "rmse_testset = np.sqrt(mean_squared_error(all_actual, all_predicted_testset))\n",
    "rmse_autoregressive = np.sqrt(mean_squared_error(all_actual, all_predicted_autoregressive))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THE TRUTH REVEALED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1️⃣ Test Set Performance (uses ACTUAL past values):\")\n",
    "print(f\"   R² = {r2_testset:.4f}\")\n",
    "print(f\"   RMSE = {rmse_testset:.2f}\")\n",
    "print(f\"   ⚠️ This is MISLEADING - you can't use actual values in simulation!\")\n",
    "\n",
    "print(f\"\\n2️⃣ Autoregressive Performance (uses PREDICTED past values):\")\n",
    "print(f\"   R² = {r2_autoregressive:.4f}\")\n",
    "print(f\"   RMSE = {rmse_autoregressive:.2f}\")\n",
    "print(f\"   ✓ This is REAL - this is what you'll get in simulation!\")\n",
    "\n",
    "print(f\"\\n📊 Performance Degradation:\")\n",
    "degradation = r2_testset - r2_autoregressive\n",
    "print(f\"   R² drops by {degradation:.4f} ({degradation/r2_testset*100:.1f}%)\")\n",
    "print(f\"   RMSE increases by {rmse_autoregressive - rmse_testset:.2f} ({(rmse_autoregressive/rmse_testset - 1)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if r2_autoregressive > 0.7:\n",
    "    print(\"✅ EXCELLENT! Autoregressive R² > 0.7\")\n",
    "    print(\"   The model works great for simulation!\")\n",
    "elif r2_autoregressive > 0.5:\n",
    "    print(\"✅ GOOD! Autoregressive R² > 0.5\")\n",
    "    print(\"   The model is usable for simulation\")\n",
    "elif r2_autoregressive > 0.3:\n",
    "    print(\"⚠️ MODERATE. Autoregressive R² > 0.3\")\n",
    "    print(\"   Model works but with limited accuracy\")\n",
    "    print(\"   Consider adding more features or trying other approaches\")\n",
    "elif r2_autoregressive > 0:\n",
    "    print(\"❌ POOR. Autoregressive R² < 0.3\")\n",
    "    print(\"   Model struggles with autoregressive generation\")\n",
    "    print(\"   Try: Approach 2 (Fourier), Approach 3 (LSTM), or use historical curves\")\n",
    "else:\n",
    "    print(\"❌ VERY POOR. Negative R²\")\n",
    "    print(\"   Model fails at autoregressive generation\")\n",
    "    print(\"   The predictions make things worse - use historical curves directly\")\n",
    "\n",
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot 1: Test set (misleading)\n",
    "axes[0].scatter(all_actual, all_predicted_testset, alpha=0.3, s=1)\n",
    "axes[0].plot([all_actual.min(), all_actual.max()], [all_actual.min(), all_actual.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Energy')\n",
    "axes[0].set_ylabel('Predicted Energy')\n",
    "axes[0].set_title(f'Test Set (uses actual past)\\nR² = {r2_testset:.4f} ⚠️ MISLEADING')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot 2: Autoregressive (real)\n",
    "axes[1].scatter(all_actual, all_predicted_autoregressive, alpha=0.3, s=1)\n",
    "axes[1].plot([all_actual.min(), all_actual.max()], [all_actual.min(), all_actual.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Energy')\n",
    "axes[1].set_ylabel('Predicted Energy')\n",
    "axes[1].set_title(f'Autoregressive (uses predicted past)\\nR² = {r2_autoregressive:.4f} ✓ REAL')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Performance Comparison: Why Test R² is Misleading', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"🎯 BOTTOM LINE: Your real performance is R² = {r2_autoregressive:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Test Curve Visualization\n",
    "\n",
    "Showing each test curve with:\n",
    "- **Black line**: Actual energy curve\n",
    "- **Blue dashed**: Prediction using actual past (teacher forcing) - misleading\n",
    "- **Red dotted**: Prediction using predicted past (autoregressive) - REAL simulation performance\n",
    "\n",
    "Each curve shows its local R² score for the autoregressive prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ALL Test Curves Individually\n",
    "print(\"=\"*80)\n",
    "print(\"INDIVIDUAL TEST CURVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Visualizing all {len(test_instance_ids)} test curves...\")\n",
    "\n",
    "# Calculate how many plots we need\n",
    "num_curves = len(test_instance_ids)\n",
    "curves_per_row = 4\n",
    "num_rows = (num_curves + curves_per_row - 1) // curves_per_row\n",
    "\n",
    "# Store R² scores for each curve\n",
    "curve_r2_scores = []\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(num_rows, curves_per_row, figsize=(20, 5*num_rows))\n",
    "if num_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx, instance_id in enumerate(test_instance_ids):\n",
    "    row = idx // curves_per_row\n",
    "    col = idx % curves_per_row\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Get instance data\n",
    "    instance_mask = df_enhanced['instance_id'] == instance_id\n",
    "    instance_data = df_enhanced[instance_mask].reset_index(drop=True)\n",
    "    \n",
    "    y_actual = instance_data['y'].values\n",
    "    \n",
    "    # Prediction with actual past (teacher forcing)\n",
    "    y_pred_testset = best_model.predict(X[instance_mask])\n",
    "    \n",
    "    # Autoregressive prediction (using predicted past)\n",
    "    y_generated = []\n",
    "    \n",
    "    for i in range(len(instance_data)):\n",
    "        row_features = instance_data.iloc[i][feature_cols].copy()\n",
    "        \n",
    "        if i > 0:\n",
    "            row_features['value_lag1'] = y_generated[-1]\n",
    "            row_features['cumsum'] = np.sum(y_generated)\n",
    "            row_features['cummean'] = np.mean(y_generated)\n",
    "            row_features['cumstd'] = np.std(y_generated) if i > 1 else 0\n",
    "            row_features['cummin'] = np.min(y_generated)\n",
    "            row_features['cummax'] = np.max(y_generated)\n",
    "            row_features['pos_x_lag1'] = row_features['norm_index'] * y_generated[-1]\n",
    "            row_features['pos_x_cummean'] = row_features['norm_index'] * np.mean(y_generated)\n",
    "            row_features['rolling_mean_3'] = np.mean(y_generated[-3:]) if len(y_generated) >= 1 else 0\n",
    "            row_features['rolling_std_3'] = np.std(y_generated[-3:]) if len(y_generated) >= 3 else 0\n",
    "            row_features['rolling_mean_5'] = np.mean(y_generated[-5:]) if len(y_generated) >= 1 else 0\n",
    "            row_features['rolling_max_5'] = np.max(y_generated[-5:]) if len(y_generated) >= 1 else 0\n",
    "            row_features['rolling_min_5'] = np.min(y_generated[-5:]) if len(y_generated) >= 1 else 0\n",
    "        \n",
    "        if i > 1:\n",
    "            row_features['value_lag2'] = y_generated[-2]\n",
    "            row_features['diff_lag1'] = y_generated[-1] - y_generated[-2]\n",
    "        \n",
    "        if i > 2:\n",
    "            row_features['diff_lag2'] = y_generated[-2] - y_generated[-3]\n",
    "            row_features['acceleration'] = (y_generated[-1] - y_generated[-2]) - (y_generated[-2] - y_generated[-3])\n",
    "        \n",
    "        if i > 2:\n",
    "            row_features['value_lag3'] = y_generated[-3]\n",
    "        \n",
    "        if i > 4:\n",
    "            row_features['value_lag5'] = y_generated[-5]\n",
    "        \n",
    "        activity_val = instance_data.iloc[i]['activity']\n",
    "        for col in X.columns:\n",
    "            if col.startswith('activity_'):\n",
    "                row_features[col] = 1 if col == f'activity_{activity_val}' else 0\n",
    "        \n",
    "        X_pred = pd.DataFrame([row_features])[X.columns]\n",
    "        y_pred = best_model.predict(X_pred)[0]\n",
    "        y_generated.append(y_pred)\n",
    "    \n",
    "    y_generated = np.array(y_generated)\n",
    "    \n",
    "    # Calculate local R² for this curve\n",
    "    local_r2 = r2_score(y_actual, y_generated)\n",
    "    curve_r2_scores.append({\n",
    "        'instance_id': instance_id,\n",
    "        'r2_autoregressive': local_r2,\n",
    "        'curve_length': len(y_actual)\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(y_actual, label='Actual', linewidth=2.5, alpha=0.8, color='black')\n",
    "    ax.plot(y_pred_testset, label='Pred (actual past)', linewidth=2, \n",
    "            alpha=0.6, linestyle='--', color='blue')\n",
    "    ax.plot(y_generated, label='Pred (predicted past)', \n",
    "            linewidth=2, alpha=0.7, linestyle=':', color='red', marker='o', markersize=3)\n",
    "    \n",
    "    # Title with R²\n",
    "    color = 'green' if local_r2 > 0.7 else 'orange' if local_r2 > 0.5 else 'red'\n",
    "    ax.set_title(f'Test Curve {idx+1}\\nAutoregressive R²={local_r2:.3f}', \n",
    "                 fontsize=11, fontweight='bold', color=color)\n",
    "    ax.set_xlabel('Time Step', fontsize=9)\n",
    "    ax.set_ylabel('Energy', fontsize=9)\n",
    "    ax.legend(fontsize=7, loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(num_curves, num_rows * curves_per_row):\n",
    "    row = idx // curves_per_row\n",
    "    col = idx % curves_per_row\n",
    "    axes[row, col].set_visible(False)\n",
    "\n",
    "plt.suptitle(f'All {num_curves} Test Curves: Autoregressive Prediction Performance', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "curve_r2_df = pd.DataFrame(curve_r2_scores)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CURVE R² STATISTICS (Autoregressive)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mean R²: {curve_r2_df['r2_autoregressive'].mean():.4f}\")\n",
    "print(f\"Median R²: {curve_r2_df['r2_autoregressive'].median():.4f}\")\n",
    "print(f\"Std R²: {curve_r2_df['r2_autoregressive'].std():.4f}\")\n",
    "print(f\"Min R²: {curve_r2_df['r2_autoregressive'].min():.4f}\")\n",
    "print(f\"Max R²: {curve_r2_df['r2_autoregressive'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nPerformance breakdown:\")\n",
    "print(f\"  Excellent (R² > 0.7): {(curve_r2_df['r2_autoregressive'] > 0.7).sum()} curves\")\n",
    "print(f\"  Good (0.5 < R² ≤ 0.7): {((curve_r2_df['r2_autoregressive'] > 0.5) & (curve_r2_df['r2_autoregressive'] <= 0.7)).sum()} curves\")\n",
    "print(f\"  Fair (0.3 < R² ≤ 0.5): {((curve_r2_df['r2_autoregressive'] > 0.3) & (curve_r2_df['r2_autoregressive'] <= 0.5)).sum()} curves\")\n",
    "\n",
    "print(f\"  Poor (R² ≤ 0.3): {(curve_r2_df['r2_autoregressive'] <= 0.3).sum()} curves\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Per-Curve R² Scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(curve_r2_df['r2_autoregressive'], bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(curve_r2_df['r2_autoregressive'].mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {curve_r2_df[\"r2_autoregressive\"].mean():.3f}')\n",
    "axes[0].axvline(curve_r2_df['r2_autoregressive'].median(), color='green', linestyle='--', \n",
    "                linewidth=2, label=f'Median: {curve_r2_df[\"r2_autoregressive\"].median():.3f}')\n",
    "axes[0].set_xlabel('Autoregressive R²', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Curves', fontsize=12)\n",
    "axes[0].set_title('Distribution of Per-Curve R² Scores', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(curve_r2_df['r2_autoregressive'], vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2),\n",
    "                whiskerprops=dict(linewidth=1.5),\n",
    "                capprops=dict(linewidth=1.5))\n",
    "axes[1].set_ylabel('Autoregressive R²', fontsize=12)\n",
    "axes[1].set_title('R² Distribution Summary', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Curve-by-curve analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Verification Summary\n",
    "\n",
    "## Train/Test Split: ✓ CORRECT\n",
    "- Split by **complete curves** (instance_id level)\n",
    "- Training curves: No mixing with test\n",
    "- Test curves: 100% unseen during training\n",
    "- Zero overlap verified\n",
    "\n",
    "## Evaluation: ✓ CORRECT\n",
    "- Each test curve evaluated **completely**\n",
    "- Autoregressive generation: Uses only predicted past values\n",
    "- Realistic simulation performance measured\n",
    "\n",
    "## Key Findings:\n",
    "1. **Test R²** (blue dashed): Uses actual past → Misleading high performance\n",
    "2. **Autoregressive R²** (red dotted): Uses predicted past → **REAL** performance\n",
    "3. Individual curve R² shows variation in prediction quality\n",
    "4. Some curves are easier to predict than others\n",
    "\n",
    "## Next Steps:\n",
    "- If autoregressive R² > 0.7: ✅ Ready for simulation\n",
    "- If autoregressive R² 0.5-0.7: ⚠️ Usable but could improve\n",
    "- If autoregressive R² < 0.5: ❌ Consider alternative approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 BUG FIX APPLIED\n",
    "\n",
    "**Issue Found:** Mismatch between training and autoregressive generation!\n",
    "\n",
    "**During Training (OLD - WRONG):**\n",
    "- Cumulative features included the **current point** we're trying to predict\n",
    "- `cumsum = values[:i+1]` ← includes values[i]\n",
    "- `rolling_mean_3 = values[i-2:i+1]` ← includes values[i]\n",
    "- `diff_lag1 = values[i] - values[i-1]` ← uses values[i]\n",
    "\n",
    "**During Autoregressive Generation:**\n",
    "- Only has access to **past predicted values**\n",
    "- `cumsum = sum(y_generated)` ← does NOT include current\n",
    "\n",
    "**Result:** Huge performance gap! Model trained with features it won't have during simulation.\n",
    "\n",
    "**Fix Applied:** Training now matches generation - only uses values BEFORE current point.\n",
    "\n",
    "**Action Required:** Re-run the training cell above to retrain with correct features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚨 CRITICAL: RETRAIN YOUR MODEL!\n",
    "\n",
    "The bug has been **fixed** in the training code, but your current model was trained with the **old broken features**.\n",
    "\n",
    "### What to do NOW:\n",
    "\n",
    "1. **Scroll up** to the training cell (search for \"APPROACH 1 FIXED\")\n",
    "2. **Re-run that cell** to train with correct features\n",
    "3. **Re-run all cells below** to see the REAL performance\n",
    "\n",
    "### What was fixed:\n",
    "\n",
    "**Training features NOW match autoregressive generation:**\n",
    "- ✅ Cumulative stats use only PAST values (not including current)\n",
    "- ✅ Rolling stats use only PAST values  \n",
    "- ✅ Diff/acceleration use only PAST values\n",
    "\n",
    "**Before fix:**\n",
    "- Model trained seeing the current value in cumulative features\n",
    "- At generation time, current value doesn't exist yet\n",
    "- **Result: Huge performance gap!**\n",
    "\n",
    "**After fix:**\n",
    "- Model trains EXACTLY as it will generate\n",
    "- No mismatch between training and generation\n",
    "- **Result: Consistent, reliable R²**\n",
    "\n",
    "### Expected outcome:\n",
    "\n",
    "Your autoregressive R² should be **much closer** to the test R² now (maybe still a small gap due to error compounding, but not as drastic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 APPROACH 2: Position-Based Regression (SIMPLER & BETTER!)\n",
    "\n",
    "Instead of autoregressive prediction (where errors compound), we'll use **position-based regression**:\n",
    "\n",
    "- Each point predicted based on its **position in the curve** (0% → 100%)\n",
    "- No dependence on previous predictions → No error compounding!\n",
    "- Similar to how `energy_profile_extractor.py` works\n",
    "\n",
    "This should give **much better results** for energy curves with consistent shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-Based Regression - Train Models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"POSITION-BASED REGRESSION - NO ERROR COMPOUNDING!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data\n",
    "variable = 'strom_PERT2_KZE_5s_energy'\n",
    "activities = ['Step-030 = Produktion']\n",
    "\n",
    "df_plot = df_expanded.copy()\n",
    "df_plot = df_plot[df_plot['activity_log'].isin(activities)]\n",
    "df_plot['timestamp_start'] = pd.to_datetime(df_plot['timestamp_start_log'])\n",
    "df_plot['datetime_energy'] = pd.to_datetime(df_plot['datetime_energy'])\n",
    "df_plot = df_plot.sort_values(['case_id_log', 'timestamp_start_log', 'datetime_energy'])\n",
    "\n",
    "# Create instance IDs\n",
    "df_plot['activity_instance_id'] = (\n",
    "    df_plot.groupby(['case_id_log', 'activity_log', 'timestamp_start_log']).ngroup()\n",
    ")\n",
    "\n",
    "# Extract and resample ALL curves to same length (median length)\n",
    "curves_data = []\n",
    "for instance_id, group in df_plot.groupby('activity_instance_id'):\n",
    "    group = group.sort_values('datetime_energy').reset_index(drop=True)\n",
    "    values = group[variable].dropna().values\n",
    "    \n",
    "    if len(values) >= 5:\n",
    "        curves_data.append({\n",
    "            'instance_id': instance_id,\n",
    "            'activity': group['activity_log'].iloc[0],\n",
    "            'original_values': values,\n",
    "            'original_length': len(values)\n",
    "        })\n",
    "\n",
    "# Calculate median length for resampling\n",
    "median_length = int(np.median([c['original_length'] for c in curves_data]))\n",
    "print(f\"\\nResampling all curves to median length: {median_length} points\")\n",
    "\n",
    "# Resample all curves to median length\n",
    "for curve in curves_data:\n",
    "    x_old = np.linspace(0, 1, len(curve['original_values']))\n",
    "    x_new = np.linspace(0, 1, median_length)\n",
    "    curve['resampled_values'] = np.interp(x_new, x_old, curve['original_values'])\n",
    "\n",
    "print(f\"Total curves: {len(curves_data)}\")\n",
    "\n",
    "# Build position-based regression dataset\n",
    "# Each row: (instance_id, position_in_curve, activity) -> energy_value\n",
    "regression_data = []\n",
    "\n",
    "for curve in curves_data:\n",
    "    for position_idx in range(median_length):\n",
    "        # Position as fraction (0 to 1)\n",
    "        position_fraction = position_idx / (median_length - 1) if median_length > 1 else 0\n",
    "        \n",
    "        regression_data.append({\n",
    "            'instance_id': curve['instance_id'],\n",
    "            'activity': curve['activity'],\n",
    "            'position_idx': position_idx,  # Absolute position (0 to median_length-1)\n",
    "            'position_fraction': position_fraction,  # Normalized position (0 to 1)\n",
    "            'curve_length': median_length,\n",
    "            'y': curve['resampled_values'][position_idx]\n",
    "        })\n",
    "\n",
    "df_regression = pd.DataFrame(regression_data)\n",
    "\n",
    "print(f\"Regression dataset: {len(df_regression)} samples\")\n",
    "print(f\"  {len(curves_data)} curves × {median_length} points\")\n",
    "\n",
    "# Prepare features (SIMPLE - just position!)\n",
    "feature_cols = ['position_idx', 'position_fraction', 'curve_length']\n",
    "X = df_regression[feature_cols].copy()\n",
    "y = df_regression['y'].copy()\n",
    "\n",
    "# Add activity encoding\n",
    "X['activity'] = df_regression['activity']\n",
    "X = pd.get_dummies(X, columns=['activity'], drop_first=True)\n",
    "\n",
    "# Split by COMPLETE CURVES (not individual points!)\n",
    "unique_instances = df_regression['instance_id'].unique()\n",
    "train_instances, test_instances = train_test_split(\n",
    "    unique_instances, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_mask = df_regression['instance_id'].isin(train_instances)\n",
    "test_mask = df_regression['instance_id'].isin(test_instances)\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "print(f\"\\nTrain/Test split:\")\n",
    "print(f\"  Train: {len(train_instances)} curves, {len(X_train)} points\")\n",
    "print(f\"  Test: {len(test_instances)} curves, {len(X_test)} points\")\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=200, max_depth=7, learning_rate=0.1,\n",
    "        subsample=0.8, random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=12, min_samples_split=5,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "results_position = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    results_position[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train R²={train_r2:.4f}, RMSE={train_rmse:.2f}\")\n",
    "    print(f\"  Test R²={test_r2:.4f}, RMSE={test_rmse:.2f}\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name_pos = max(results_position.keys(), key=lambda k: results_position[k]['test_r2'])\n",
    "best_model_pos = results_position[best_model_name_pos]['model']\n",
    "\n",
    "print(f\"\\n✓ Best model: {best_model_name_pos}\")\n",
    "print(f\"  Test R² = {results_position[best_model_name_pos]['test_r2']:.4f}\")\n",
    "\n",
    "# Store for later use\n",
    "median_curve_length = median_length\n",
    "feature_columns_pos = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Test Curves - Position-Based Approach\n",
    "print(\"=\"*80)\n",
    "print(\"POSITION-BASED PREDICTION - TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_curves = [c for c in curves_data if c['instance_id'] in test_instances]\n",
    "num_test = len(test_curves)\n",
    "\n",
    "print(f\"Predicting {num_test} test curves...\")\n",
    "\n",
    "# Calculate grid\n",
    "curves_per_row = 4\n",
    "num_rows = (num_test + curves_per_row - 1) // curves_per_row\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, curves_per_row, figsize=(20, 5*num_rows))\n",
    "if num_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "curve_r2_scores_pos = []\n",
    "\n",
    "for idx, curve in enumerate(test_curves[:num_test]):\n",
    "    # Get actual values (resampled)\n",
    "    y_actual = curve['resampled_values']\n",
    "    \n",
    "    # Predict entire curve using position-based model\n",
    "    X_curve = pd.DataFrame({\n",
    "        'position_idx': np.arange(median_length),\n",
    "        'position_fraction': np.linspace(0, 1, median_length),\n",
    "        'curve_length': median_length,\n",
    "    })\n",
    "    \n",
    "    # Add activity encoding\n",
    "    activity_val = curve['activity']\n",
    "    for col in feature_columns_pos:\n",
    "        if col.startswith('activity_'):\n",
    "            X_curve[col] = 1 if col == f'activity_{activity_val}' else 0\n",
    "        elif col not in X_curve.columns:\n",
    "            X_curve[col] = 0\n",
    "    \n",
    "    # Ensure column order matches\n",
    "    X_curve = X_curve[feature_columns_pos]\n",
    "    \n",
    "    y_pred = best_model_pos.predict(X_curve)\n",
    "    \n",
    "    # Calculate R²\n",
    "    local_r2 = r2_score(y_actual, y_pred)\n",
    "    curve_r2_scores_pos.append({\n",
    "        'instance_id': curve['instance_id'],\n",
    "        'r2': local_r2,\n",
    "        'curve_length': len(y_actual)\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    ax.plot(y_actual, label='Actual', linewidth=2.5, alpha=0.8, color='black')\n",
    "    ax.plot(y_pred, label='Predicted', linewidth=2, alpha=0.7, \n",
    "            linestyle='--', color='green', marker='o', markersize=3)\n",
    "    \n",
    "    color = 'green' if local_r2 > 0.7 else 'orange' if local_r2 > 0.5 else 'red'\n",
    "    ax.set_title(f'Test Curve {idx+1}\\nR²={local_r2:.3f}', \n",
    "                 fontsize=11, fontweight='bold', color=color)\n",
    "    ax.set_xlabel('Position Index', fontsize=9)\n",
    "    ax.set_ylabel('Energy', fontsize=9)\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(num_test, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle(f'Position-Based Predictions: {best_model_name_pos}', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "curve_r2_df_pos = pd.DataFrame(curve_r2_scores_pos)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POSITION-BASED R² STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mean R²: {curve_r2_df_pos['r2'].mean():.4f}\")\n",
    "print(f\"Median R²: {curve_r2_df_pos['r2'].median():.4f}\")\n",
    "print(f\"Std R²: {curve_r2_df_pos['r2'].std():.4f}\")\n",
    "print(f\"Min R²: {curve_r2_df_pos['r2'].min():.4f}\")\n",
    "print(f\"Max R²: {curve_r2_df_pos['r2'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nPerformance breakdown:\")\n",
    "print(f\"  Excellent (R² > 0.7): {(curve_r2_df_pos['r2'] > 0.7).sum()} curves\")\n",
    "print(f\"  Good (0.5 < R² ≤ 0.7): {((curve_r2_df_pos['r2'] > 0.5) & (curve_r2_df_pos['r2'] <= 0.7)).sum()} curves\")\n",
    "print(f\"  Fair (0.3 < R² ≤ 0.5): {((curve_r2_df_pos['r2'] > 0.3) & (curve_r2_df_pos['r2'] <= 0.5)).sum()} curves\")\n",
    "print(f\"  Poor (R² ≤ 0.3): {(curve_r2_df_pos['r2'] <= 0.3).sum()} curves\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall R² (all test points combined)\n",
    "all_actual_pos = []\n",
    "all_pred_pos = []\n",
    "for curve in test_curves:\n",
    "    all_actual_pos.extend(curve['resampled_values'])\n",
    "    \n",
    "    X_curve = pd.DataFrame({\n",
    "        'position_idx': np.arange(median_length),\n",
    "        'position_fraction': np.linspace(0, 1, median_length),\n",
    "        'curve_length': median_length,\n",
    "    })\n",
    "    activity_val = curve['activity']\n",
    "    for col in feature_columns_pos:\n",
    "        if col.startswith('activity_'):\n",
    "            X_curve[col] = 1 if col == f'activity_{activity_val}' else 0\n",
    "        elif col not in X_curve.columns:\n",
    "            X_curve[col] = 0\n",
    "    X_curve = X_curve[feature_columns_pos]\n",
    "    y_pred = best_model_pos.predict(X_curve)\n",
    "    all_pred_pos.extend(y_pred)\n",
    "\n",
    "overall_r2_pos = r2_score(all_actual_pos, all_pred_pos)\n",
    "overall_rmse_pos = np.sqrt(mean_squared_error(all_actual_pos, all_pred_pos))\n",
    "\n",
    "print(f\"\\n🎯 OVERALL TEST PERFORMANCE:\")\n",
    "print(f\"   R² = {overall_r2_pos:.4f}\")\n",
    "print(f\"   RMSE = {overall_rmse_pos:.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Why Position-Based Works Better\n",
    "\n",
    "**Autoregressive Approach:**\n",
    "- Predicts point `i` based on points `i-1, i-2, ...`\n",
    "- Error at point `i` affects point `i+1` → Errors compound!\n",
    "- Like dominos falling - one mistake cascades\n",
    "\n",
    "**Position-Based Approach:**\n",
    "- Predicts point `i` based on its **position** (e.g., \"I'm 30% through the curve\")\n",
    "- Each point predicted independently\n",
    "- No error compounding - each prediction is fresh!\n",
    "\n",
    "**For energy curves with consistent shapes, position-based is MUCH better!**\n",
    "\n",
    "The model learns: \"At position 30% in this activity, energy is typically X kW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Generate Energy Curves for Simulation\n",
    "def generate_energy_curve_position_based(\n",
    "    activity, \n",
    "    desired_length, \n",
    "    model=None, \n",
    "    median_length=None,\n",
    "    feature_columns=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate an energy curve using position-based regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    activity : str\n",
    "        Activity name (e.g., 'Step-030 = Produktion')\n",
    "    desired_length : int\n",
    "        Desired number of points in output curve\n",
    "    model : trained model, optional\n",
    "        Defaults to best_model_pos\n",
    "    median_length : int, optional\n",
    "        Median curve length used in training\n",
    "    feature_columns : list, optional\n",
    "        Feature column names from training\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.array : Generated energy curve with desired_length points\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = best_model_pos\n",
    "    if median_length is None:\n",
    "        median_length = median_curve_length\n",
    "    if feature_columns is None:\n",
    "        feature_columns = feature_columns_pos\n",
    "    \n",
    "    # Step 1: Generate curve at median length (what model was trained on)\n",
    "    X_latent = pd.DataFrame({\n",
    "        'position_idx': np.arange(median_length),\n",
    "        'position_fraction': np.linspace(0, 1, median_length),\n",
    "        'curve_length': median_length,\n",
    "    })\n",
    "    \n",
    "    # Add activity encoding\n",
    "    for col in feature_columns:\n",
    "        if col.startswith('activity_'):\n",
    "            X_latent[col] = 1 if col == f'activity_{activity}' else 0\n",
    "        elif col not in X_latent.columns:\n",
    "            X_latent[col] = 0\n",
    "    \n",
    "    X_latent = X_latent[feature_columns]\n",
    "    \n",
    "    # Predict at median length\n",
    "    latent_curve = model.predict(X_latent)\n",
    "    \n",
    "    # Step 2: Resample to desired length\n",
    "    if desired_length != median_length:\n",
    "        x_old = np.linspace(0, 1, median_length)\n",
    "        x_new = np.linspace(0, 1, desired_length)\n",
    "        final_curve = np.interp(x_new, x_old, latent_curve)\n",
    "    else:\n",
    "        final_curve = latent_curve\n",
    "    \n",
    "    return final_curve\n",
    "\n",
    "# Test the function\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING CURVE GENERATION FUNCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_curve = generate_energy_curve_position_based(\n",
    "    activity='Step-030 = Produktion',\n",
    "    desired_length=100\n",
    ")\n",
    "\n",
    "print(f\"Generated curve with {len(test_curve)} points\")\n",
    "print(f\"  Min: {test_curve.min():.2f}\")\n",
    "print(f\"  Max: {test_curve.max():.2f}\")\n",
    "print(f\"  Mean: {test_curve.mean():.2f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(test_curve, linewidth=2, color='green')\n",
    "plt.title('Example Generated Energy Curve (100 points)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Time Point')\n",
    "plt.ylabel('Energy (kW)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Curve generation function ready for simulation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
